{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Krishanu-Saha/data-science/blob/main/TRAIN_HEALTH_INSURANCE_CROSS_SELL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - HEALTH_INSURANCE_CROSS_SELL\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Classification\n",
        "##### **Contribution**    - Individual, BY KRISHANU SAHA\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The project titled \"Health Insurance Cross Sell\" is a data science project aimed at developing a predictive model that can accurately predict which customers of a health insurance company are likely to purchase vehicle insurance. The project was carried out using Python programming language and various libraries such as Pandas, Numpy, Seaborn, and Scikit-learn.\n",
        "\n",
        "The dataset contains information about 381,109 customers, including their age, gender, driving license status, region, previously insured status, vehicle age, vehicle damage, annual premium, and policy sales channel. The objective of the project was to build a classification model that can predict if a customer will buy vehicle insurance or not, based on the given set of attributes.\n",
        "\n",
        "The project involved several steps, starting with data cleaning and preprocessing. The dataset contained missing values and categorical variables that needed to be encoded into numerical values. The next step was to perform exploratory data analysis to gain insights into the data and identify any patterns or relationships between the variables.\n",
        "\n",
        "After data cleaning and EDA, the dataset was split into training and testing sets, and several machine learning algorithms were applied to the training data to develop a predictive model. The algorithms used in this project included logistic regression,  random forest,  and XGBoost. The model was trained using various hyperparameters, and the best hyperparameters were selected using cross-validation techniques.\n",
        "\n",
        "The evaluation metrics used to measure the performance of the model included accuracy, precision, recall, F1 score, and ROC-AUC curve. The results showed that the gradient boosting algorithm performed the best, with an accuracy score of 0.87 and an F1 score of 0.56.\n",
        "\n",
        "In addition to developing a predictive model, the project also provided several business insights that could be useful for the health insurance company. For example, the analysis revealed that customers who had previously purchased vehicle insurance were more likely to buy it again, suggesting that the company should focus on retaining its existing customers. The analysis also showed that customers who had damaged their vehicles in the past were more likely to purchase insurance, suggesting that the company should target this segment with specific marketing campaigns.\n",
        "\n",
        "Overall, the project demonstrated the potential of data science to extract insights from large datasets and develop predictive models that can help businesses make informed decisions. The project also highlighted the importance of data cleaning and exploratory data analysis in ensuring the accuracy and relevance of the model. The results of this project could be used by the health insurance company to improve its sales and marketing strategies, retain customers, and increase revenue."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/Krishanu-Saha/data-science/blob/main/TRAIN_HEALTH_INSURANCE_CROSS_SELL.ipynb"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Our client is an Insurance company that has provided Health Insurance to its customers now they need your help in building a model to predict whether the policyholders (customers) from past year will also be interested in Vehicle Insurance provided by the company.\n",
        "\n",
        "An insurance policy is an arrangement by which a company undertakes to provide a guarantee of compensation for specified loss, damage, illness, or death in return for the payment of a specified premium. A premium is a sum of money that the customer needs to pay regularly to an insurance company for this guarantee.\n",
        "\n",
        "For example, you may pay a premium of Rs. 5000 each year for a health insurance cover of Rs. 200,000/- so that if, God forbid, you fall ill and need to be hospitalised in that year, the insurance provider company will bear the cost of hospitalisation etc. for upto Rs. 200,000. Now if you are wondering how can company bear such high hospitalisation cost when it charges a premium of only Rs. 5000/-, that is where the concept of probabilities comes in picture. For example, like you, there may be 100 customers who would be paying a premium of Rs. 5000 every year, but only a few of them (say 2-3) would get hospitalised that year and not everyone. This way everyone shares the risk of everyone else.\n",
        "\n",
        "Just like medical insurance, there is vehicle insurance where every year customer needs to pay a premium of certain amount to insurance provider company so that in case of unfortunate accident by the vehicle, the insurance provider company will provide a compensation (called ‘sum assured’) to the customer.\n",
        "\n",
        "Building a model to predict whether a customer would be interested in Vehicle Insurance is extremely helpful for the company because it can then accordingly plan its communication strategy to reach out to those customers and optimise its business model and revenue.\n",
        "\n",
        "Now, in order to predict, whether the customer would be interested in Vehicle insurance, we have information about demographics (gender, age, region code type), Vehicles (Vehicle Age, Damage), Policy (Premium, sourcing channel) etc.**"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ATTRIBUTE INFORMATION"
      ],
      "metadata": {
        "id": "bWozRdSoqzW_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "id : Unique ID for the customer\n",
        "\n",
        "Gender : Gender of the customer\n",
        "\n",
        "Age : Age of the customer\n",
        "\n",
        "Driving_License 0 : Customer does not have DL, 1 : Customer already has DL\n",
        "\n",
        "Region_Code : Unique code for the region of the customer\n",
        "\n",
        "Previously_Insured : 1 : Customer already has Vehicle Insurance, 0 : Customer doesn't have Vehicle Insurance\n",
        "\n",
        "Vehicle_Age : Age of the Vehicle\n",
        "\n",
        "Vehicle_Damage :1 : Customer got his/her vehicle damaged in the past. 0 : Customer didn't get his/her vehicle damaged in the past.\n",
        "\n",
        "Annual_Premium : The amount customer needs to pay as premium in the year\n",
        "\n",
        "PolicySalesChannel : Anonymized Code for the channel of outreaching to the customer ie. Different Agents, Over Mail, Over Phone, In Person, etc.\n",
        "\n",
        "Vintage : Number of Days, Customer has been associated with the company\n",
        "\n",
        "Response : 1 : Customer is interested, 0 : Customer is not interested"
      ],
      "metadata": {
        "id": "3eRgfb7yq1AL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "# Basic\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Plotation\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "%matplotlib inline\n",
        "\n",
        "import math\n",
        "\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from xgboost import XGBClassifier\n",
        "from xgboost import XGBRFClassifier\n",
        "from sklearn.tree import export_graphviz\n",
        "\n",
        "!pip install shap==0.40.0\n",
        "import shap\n",
        "import graphviz\n",
        "sns.set_style('darkgrid')\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "# Miscellaneous\n",
        "import time\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install missingno"
      ],
      "metadata": {
        "id": "yqUrrjhUtVeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import missingno as msno"
      ],
      "metadata": {
        "id": "iixXlYRIxIdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GXa24fq-xIaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "_4eJ8CAytWQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "data = pd.read_csv('/content/drive/MyDrive/Almabetter /project/CLASSIFICATION/TRAIN-HEALTH INSURANCE CROSS SELL PREDICTION .csv')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "data.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns\n",
        "data.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "data.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "len(data[data.duplicated()])\n"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "No duplicate values found."
      ],
      "metadata": {
        "id": "a7n9fVwPwK-q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "data.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "No column has missing/Null values ."
      ],
      "metadata": {
        "id": "cfvaC6bNwPoB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "msno.matrix(data)"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The shape of the dataset is (381109, 12),\n",
        "\n",
        "No record is duplicated,\n",
        "\n",
        "No column has missing or null values."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "list(data.columns)"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "data.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "data.nunique()"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "insurance_data = data.copy()"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Checking the distribution of the Response variable to see if it is balanced. This can be done by calculating the percentage of customers who are interested in purchasing insurance and comparing it to the percentage who are not interested.**"
      ],
      "metadata": {
        "id": "e635yGaA0fXd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "counts = insurance_data['Response'].value_counts()\n",
        "\n",
        "percentage_interested = counts[1]/sum(counts)*100\n",
        "\n",
        "percentage_not_interested = counts[0]/sum(counts)*100\n",
        "\n",
        "# Print the results\n",
        "print(f\"Percentage of customers who are interested in purchasing insurance: {percentage_interested:.2f}%\")\n",
        "print(f\"Percentage of customers who are not interested in purchasing insurance: {percentage_not_interested:.2f}%\")"
      ],
      "metadata": {
        "id": "lFMLBSWW0exE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Percentage of customers who are interested in purchasing insurance: 12.26%\n",
        "\n",
        "Percentage of customers who are not interested in purchasing insurance: 87.74%"
      ],
      "metadata": {
        "id": "Kvq0fhmLAOvr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Calculating the average age of customers who are interested in purchasing insurance and compare it to the average age of customers who are not interested.**"
      ],
      "metadata": {
        "id": "_JzUW8T0AqZ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "interested_customers = insurance_data[insurance_data['Response'] == 1]\n",
        "mean_age_interested = interested_customers['Age'].mean()\n",
        "\n",
        "not_interested_customers = insurance_data[insurance_data['Response'] == 0]\n",
        "mean_age_not_interested = not_interested_customers['Age'].mean()\n",
        "\n",
        "# Print the results\n",
        "print(f\"Average age of customers who are interested in purchasing insurance: {mean_age_interested:.2f} years\")\n",
        "print(f\"Average age of customers who are not interested in purchasing insurance: {mean_age_not_interested:.2f} years\")"
      ],
      "metadata": {
        "id": "PAmjA3BUAUMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Average age of customers who are interested in purchasing insurance: 43.44 years\n",
        "\n",
        "Average age of customers who are not interested in purchasing insurance: 38.18 years"
      ],
      "metadata": {
        "id": "hBNpTzd3Bj20"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Calculating the average annual premium for customers who are interested in purchasing insurance and compare it to the average annual premium for customers who are not interested.**"
      ],
      "metadata": {
        "id": "VlL2bzBQCHkm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the average annual premium of customers who are interested in purchasing insurance\n",
        "interested_customers = insurance_data[insurance_data['Response'] == 1]\n",
        "mean_premium_interested = interested_customers['Annual_Premium'].mean()\n",
        "\n",
        "# Calculate the average annual premium of customers who are not interested in purchasing insurance\n",
        "not_interested_customers = insurance_data[insurance_data['Response'] == 0]\n",
        "mean_premium_not_interested = not_interested_customers['Annual_Premium'].mean()\n",
        "\n",
        "# Print the results\n",
        "print(f\"Average annual premium of customers who are interested in purchasing insurance: ${mean_premium_interested:.2f}\")\n",
        "print(f\"Average annual premium of customers who are not interested in purchasing insurance: ${mean_premium_not_interested:.2f}\")\n"
      ],
      "metadata": {
        "id": "xwPbIR2BBl01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Average annual premium of customers who are interested in purchasing insurance: $31604.09\n",
        "\n",
        "Average annual premium of customers who are not interested in purchasing insurance: $30419.16"
      ],
      "metadata": {
        "id": "NqVdAlr5B-pn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Checking the percentage of customers who have a driving license by calculating the proportion of customers with a driving license.**"
      ],
      "metadata": {
        "id": "lve55WQGCRKh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the percentage of customers who have a driving license\n",
        "license_counts = insurance_data['Driving_License'].value_counts()\n",
        "percentage_licensed = license_counts[1] / sum(license_counts) * 100\n",
        "\n",
        "# Print the result\n",
        "print(f\"Percentage of customers with a driving license: {percentage_licensed:.2f}%\")\n"
      ],
      "metadata": {
        "id": "1i8SfQ67CVFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Percentage of customers with a driving license: 99.79%\n"
      ],
      "metadata": {
        "id": "mnpe_9_nCgL0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Checking the percentage of customers who have previously purchased insurance by calculating the proportion of customers who have previously purchased insurance.**"
      ],
      "metadata": {
        "id": "Rs_J5GHVCuvd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the percentage of customers who have previously purchased insurance\n",
        "prev_insurance_counts = insurance_data['Previously_Insured'].value_counts()\n",
        "percentage_prev_insured = prev_insurance_counts[1] / sum(prev_insurance_counts) * 100\n",
        "\n",
        "# Print the result\n",
        "print(f\"Percentage of customers who have previously purchased insurance: {percentage_prev_insured:.2f}%\")\n"
      ],
      "metadata": {
        "id": "35UtynJICkse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Percentage of customers who have previously purchased insurance: 45.82%\n"
      ],
      "metadata": {
        "id": "GY4MPFnODEGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Calculating the average number of days since policy start for customers who are interested in purchasing insurance and compare it to the average number of days for customers who are not interested.**"
      ],
      "metadata": {
        "id": "mmphoEUyDD80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the average vintage for customers who are interested in purchasing insurance\n",
        "interested_customers = insurance_data[insurance_data['Response'] == 1]\n",
        "mean_vintage_interested = interested_customers['Vintage'].mean()\n",
        "\n",
        "# Calculate the average vintage for customers who are not interested in purchasing insurance\n",
        "not_interested_customers = insurance_data[insurance_data['Response'] == 0]\n",
        "mean_vintage_not_interested = not_interested_customers['Vintage'].mean()\n",
        "\n",
        "# Print the results\n",
        "print(f\"Average vintage for customers who are interested in purchasing insurance: {mean_vintage_interested:.2f}\")\n",
        "print(f\"Average vintage for customers who are not interested in purchasing insurance: {mean_vintage_not_interested:.2f}\")\n"
      ],
      "metadata": {
        "id": "rMhlln9LDLy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Average vintage for customers who are interested in purchasing insurance: 154.11\n",
        "\n",
        "Average vintage for customers who are not interested in purchasing insurance: 154.38"
      ],
      "metadata": {
        "id": "u0kTs2wDEQ4q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Checking the percentage of customers who have had vehicle damage in the past by calculating the proportion of customers who have had vehicle damage.**"
      ],
      "metadata": {
        "id": "v9eIKpmvEcKn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the proportion of customers who have had vehicle damage\n",
        "vehicle_damage_count = insurance_data['Vehicle_Damage'].value_counts()\n",
        "vehicle_damage_prop = vehicle_damage_count[1] / insurance_data.shape[0]\n",
        "\n",
        "# Print the result\n",
        "print(f\"Percentage of customers who have had vehicle damage in the past: {vehicle_damage_prop*100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "WdT5fmEmEova"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Percentage of customers who have had vehicle damage in the past: 49.51%"
      ],
      "metadata": {
        "id": "Xd-It7nTEtTL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Checking the distribution of the vehicle_age variable by calculating the percentage of customers in each category.**\n"
      ],
      "metadata": {
        "id": "MydH3SjHEw6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the percentage of customers in each category of vehicle age\n",
        "vehicle_age_count = insurance_data['Vehicle_Age'].value_counts()\n",
        "vehicle_age_prop = vehicle_age_count / insurance_data.shape[0]*100\n",
        "\n",
        "# Print the result\n",
        "print(f\"Percentage of customers in each category of vehicle age:\\n{vehicle_age_prop}%\")\n"
      ],
      "metadata": {
        "id": "r9l10r6bFe1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Percentage of customers in each category of vehicle age:\n",
        "\n",
        "'1-2 Year     52.561341'\n",
        "\n",
        "'< 1 Year     43.238549'\n",
        "\n",
        "'> 2 Years     4.200111'\n"
      ],
      "metadata": {
        "id": "IpxiQDxcHNRb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Calculating the correlation between the interested_in_policy variable and other variables in the dataset using the corr function in pandas.**"
      ],
      "metadata": {
        "id": "kKxZ69MVI_If"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the correlation between the interested_in_policy variable and other variables\n",
        "corr = insurance_data.corr()['Response'].sort_values()\n",
        "\n",
        "# Print the result\n",
        "print(f\"Correlation between the Response variable and other variables:\\n{corr}\")\n"
      ],
      "metadata": {
        "id": "NrTgW7gIG_6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlation between the interested_in_policy variable and other variables:\n",
        "\n",
        "Previously_Insured     -0.341170\n",
        "\n",
        "Policy_Sales_Channel   -0.139042\n",
        "\n",
        "id                     -0.001368\n",
        "\n",
        "Vintage                -0.001050\n",
        "\n",
        "Driving_License         0.010155\n",
        "\n",
        "Region_Code             0.010570\n",
        "\n",
        "Annual_Premium          0.022575\n",
        "\n",
        "Age                     0.111147\n",
        "\n",
        "Response                1.000000"
      ],
      "metadata": {
        "id": "iNmHFC60Hz5E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the given dataset, several manipulations were performed to gain insights into customer behavior. It was found that only 12.26% of customers were interested in purchasing insurance, while the remaining 87.74% were not interested. The average age of customers who were interested in purchasing insurance was 43.44 years, which was higher than the average age of customers who were not interested (38.18 years). The average annual premium for customers who were interested in purchasing insurance was ($31604.09),\n",
        "\n",
        "which was slightly higher than the average annual premium for customers who were not interested ($30419.16).\n",
        "\n",
        "Almost all customers (99.79%) had a driving license, and 45.82% of customers had previously purchased insurance. The average vintage for customers who were interested in purchasing insurance was 154.11 days, while for customers who were not interested, it was 154.38 days. Almost half of the customers (49.51%) had vehicle damage in the past, and the majority of customers had a vehicle age between 1-2 years (52.56%) or less than 1 year (43.24%).\n",
        "\n",
        "Finally, the correlation between the Response variable and other variables was calculated, revealing that the most significant negative correlation was with the previously_insured variable (-0.341170), followed by the policy_sales_channel variable (-0.139042). The variables that had a positive correlation with Response were age (0.111147) and annual_premium (0.022575), albeit a weak one. There was a negligible correlation between the id, vintage, and driving_license variables with interested_in_policy, while the region_code variable had a slight positive correlation (0.010570)."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "insurance_df = data.copy()"
      ],
      "metadata": {
        "id": "a9gYbtGcgvJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Distribution of policyholders by age and gender**"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new DataFrame with only the columns we need\n",
        "df_age_gender = insurance_df[['Gender', 'Age']]\n",
        "\n",
        "# Create separate DataFrames for males and females\n",
        "df_males = df_age_gender[df_age_gender['Gender'] == 'Male']\n",
        "df_females = df_age_gender[df_age_gender['Gender'] == 'Female']\n",
        "\n",
        "# Plot histograms of age for each gender\n",
        "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
        "sns.histplot(data=df_males, x='Age', ax=axs[0], color='blue', alpha=0.5, bins=20)\n",
        "sns.histplot(data=df_females, x='Age', ax=axs[1], color='pink', alpha=0.5, bins=20)\n",
        "\n",
        "# Set plot titles and labels\n",
        "axs[0].set_title('Age distribution for males')\n",
        "axs[0].set_xlabel('Age')\n",
        "axs[0].set_ylabel('Count')\n",
        "axs[1].set_title('Age distribution for females')\n",
        "axs[1].set_xlabel('Age')\n",
        "axs[1].set_ylabel('Count')\n",
        "fig.suptitle('Distribution of policyholders by age and gender')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The histogram is a popular graphing tool. It is used to summarize discrete or continuous data that are measured on an interval scale. It is often used to illustrate the major features of the distribution of the data in a convenient form."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The counts of 20-30 age group in male and female are the highest."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "5lAvcCp_0Ok3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The fact that the highest counts of the 20-30 age group are in both male and female categories can be a positive business impact for insurance companies. It suggests that targeting this age group with insurance products can be profitable as there is a higher likelihood of attracting customers within this demographic.\n",
        "\n",
        "Furthermore, the insight that customers who are interested in purchasing insurance have a higher average age and are willing to pay a slightly higher annual premium than customers who are not interested can also be leveraged by insurance companies. It suggests that there may be a market for more expensive and comprehensive insurance products, which may result in higher revenue for the companies."
      ],
      "metadata": {
        "id": "1kdPUaPH0Ok4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Relationship between age and response to insurance:**"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a box plot\n",
        "sns.boxplot(x=\"Response\", y=\"Age\", data=insurance_df)"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Box plots are used to show distributions of numeric data values, especially when you want to compare them between multiple groups. They are built to provide high-level information at a glance, offering general information about a group of data's symmetry, skew, variance, and outliers."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mean age for Response 1 is around 42 and Response 0 is around 35."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insurance companies can leverage this insight to design insurance products that are more appealing to customers in their 40s and to target their marketing efforts towards this age group. This insight suggests that customers in their 40s may have higher disposable income and may be more willing to pay a slightly higher premium for comprehensive insurance products.\n",
        "\n",
        "Therefore, this insight can help create a positive business impact by enabling insurance companies to better understand the age demographic of their potential customers and to design products and marketing campaigns that target this demographic more effectively.\n",
        "\n",
        "However, this insight could also potentially lead to negative growth if insurance companies solely focus on targeting customers in their 40s and ignore other age groups. It is essential for insurance companies to carefully analyze their customer data to ensure they are targeting all age demographics effectively and not overlooking any potential customer segments."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Distribution of policyholders by region**"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of policyholders in each region\n",
        "region_count = insurance_df['Region_Code'].value_counts()\n",
        "\n",
        "# Plot the distribution of policyholders by region\n",
        "plt.figure(figsize = (15,8))\n",
        "plt.bar(region_count.index, region_count.values)\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('Region Code')\n",
        "plt.ylabel('Number of Policyholders')\n",
        "plt.title('Distribution of Policyholders by Region')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Finding Out the region with maximum occurance\n",
        "max_region = insurance_df['Region_Code'].mode()[0]\n",
        "\n",
        "#Counting the number of customers in maximum occuring region\n",
        "num_customers = insurance_df[insurance_df['Region_Code'] == max_region]['Region_Code'].count()\n",
        "\n",
        "#Printing the maximum occuring region and corresponding number of customers in that region\n",
        "print(f\"The region code with the maximum customers is {max_region} with {num_customers} customers.\")"
      ],
      "metadata": {
        "id": "B93HhcwnpTcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It allows you to compare different sets of data among different groups easily. It instantly demonstrates this relationship using two axes, where the categories are on one axis and the various values are on the other. A bar graph can also illustrate important changes in data throughout a period of time."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The region code with the maximum customers is 28.0 with 106415 customers."
      ],
      "metadata": {
        "id": "hDJjT05HpiWU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the given information, the insight that the region code with the maximum customers is 28.0 with 106,415 customers may not have a significant positive or negative impact on business growth.\n",
        "\n",
        "This insight could be useful for insurance companies to understand which regions have a higher concentration of potential customers. However, it is important to note that this insight does not provide any information on the demographic characteristics or purchasing behaviors of customers in this region.\n",
        "\n",
        "Therefore, while this insight may be interesting from a descriptive perspective, it may not necessarily help insurance companies make strategic decisions that can lead to a positive impact on business growth. To create a positive impact on business growth, insurance companies may need to combine this insight with other insights, such as demographic characteristics, purchasing behaviors, and preferences of customers in this region, to develop effective marketing and product strategies"
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Comparison of policyholders who responded to insurance with those who did not:**"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Group the data by Response and calculate the mean age for each group\n",
        "age_response = insurance_df.groupby('Response')['Age'].mean()\n",
        "\n",
        "# Create a bar chart of the mean age for each group\n",
        "sns.barplot(x=age_response.index, y=age_response.values)\n",
        "plt.title('Comparison of mean age for customers who responded to insurance')\n",
        "plt.xlabel('Response')\n",
        "plt.ylabel('Mean Age')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It allows you to compare different sets of data among different groups easily. It instantly demonstrates this relationship using two axes, where the categories are on one axis and the various values are on the other. A bar graph can also illustrate important changes in data throughout a period of time."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "people with age over 40 are likely to Response to the insurance buying."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insight that people over the age of 40 are likely to respond to insurance buying can potentially have a positive business impact for insurance companies. By targeting this demographic with tailored marketing campaigns and products that cater to their needs, insurance companies can increase their sales and revenue.\n",
        "\n",
        "However, it is important to note that not all insights lead to positive business impacts. For example, if an analysis of customer data reveals that a significant portion of customers are dissatisfied with a company's products or services, this insight may lead to negative growth if the company does not take appropriate action to address the issue."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "    # Categorizing Age feature\n",
        "    age_bins = [0, 30, 65, np.inf]\n",
        "    age_labels = ['YoungAdult', 'MiddleAdult', 'Senior']\n",
        "    insurance_df['Age_Group'] = pd.cut(insurance_df['Age'], bins=age_bins, labels=age_labels)\n",
        "\n",
        "    # Categorizing Policy_Sales_Channel feature\n",
        "    channel_bins = [0, 41, 81, 121,165]\n",
        "    channel_labels = ['Channel_A','Channel_B', 'Channel_C', 'Channel_D']\n",
        "    insurance_df['Policy_Sales_Channel_Categorical'] = pd.cut(insurance_df['Policy_Sales_Channel'], bins=channel_bins, labels=channel_labels)\n",
        "\n",
        "    # Categorizing Region_Code feature\n",
        "    region_bins = [0,11,21,31,41,53]\n",
        "    region_labels = ['Region_E','Region_D','Region_C', 'Region_B', 'Region_A']\n",
        "    insurance_df['Region_Code_Categorical'] = pd.cut(insurance_df['Region_Code'], bins=region_bins, labels=region_labels)\n",
        "\n"
      ],
      "metadata": {
        "id": "Zf71AKe-U1Vy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Distribution of Responses by Age Group,  Region Code and Policy Sales Channel**"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the figure with 1 row and 3 columns, and a size of 22 x 5 inches\n",
        "fig, axes = plt.subplots(1, 3, figsize=(22, 5))\n",
        "\n",
        "# Plot a countplot of Age_Group, colored by Response, in the first column\n",
        "sns.countplot(ax=axes[0], x='Age_Group', data=insurance_df, hue='Response')\n",
        "axes[0].set_xlabel('Age Group', fontsize=14)\n",
        "axes[0].set_ylabel('Count', fontsize=14)\n",
        "axes[0].set_title('Distribution of Responses by Age Group', fontsize=15)\n",
        "\n",
        "# Plot a countplot of Region_Code_Categorical, colored by Response, in the second column\n",
        "sns.countplot(ax=axes[1], x='Region_Code_Categorical', data=insurance_df, hue='Response')\n",
        "axes[1].set_xlabel('Region Code', fontsize=14)\n",
        "axes[1].set_ylabel('Count', fontsize=14)\n",
        "axes[1].set_title('Distribution of Responses by Region Code', fontsize=15)\n",
        "\n",
        "# Plot a countplot of Policy_Sales_Channel_Categorical, colored by Response, in the third column\n",
        "sns.countplot(ax=axes[2], x='Policy_Sales_Channel_Categorical', data=insurance_df, hue='Response')\n",
        "axes[2].set_xlabel('Policy Sales Channel', fontsize=14)\n",
        "axes[2].set_ylabel('Count', fontsize=14)\n",
        "axes[2].set_title('Distribution of Responses by Policy Sales Channel', fontsize=15)\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I picked the countplot chart specifically for this example because we are interested in visualizing the distribution of a categorical variable (\"Response\") by other categorical variables (\"Age_Group\", \"Region_Code_Categorical\", and \"Policy_Sales_Channel_Categorical\").\n",
        "\n",
        "The countplot is a good choice for this type of data because it shows the count of each category in a bar chart format, which makes it easy to compare the frequency of each category across different groups. Additionally, by using the hue parameter to color the bars by the \"Response\" variable, we can see the distribution of positive and negative responses within each category, which gives us more insight into the relationship between the different variables."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The countplots show that the middle adult age group has the highest number of responses, followed by the young adult and senior groups.\n",
        "\n",
        "In terms of region codes, Region_C has the highest count, followed by Region_E.\n",
        "\n",
        "Lastly, the Policy Sales Channel D has the highest count compared to other channels."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gained insights from the countplots may help create a positive business impact in several ways.\n",
        "\n",
        "For example, knowing that the middle adult age group has the highest number of responses could help the business to tailor their marketing strategies to target this group more effectively. Additionally, understanding which regions have the highest number of responses could help the business to allocate their resources more efficiently in those areas.\n",
        "\n",
        "Similarly, knowing which policy sales channels have the highest count could help the business to focus their efforts on those channels, while possibly rethinking their strategy for channels that have a lower count.\n",
        "\n",
        "Overall, having a better understanding of the relationship between these variables and the customer response rate could help the business to optimize their approach and improve their bottom line."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Response V/S Gender**"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a countplot using catplot to show the distribution of response by gender\n",
        "sns.catplot(x=\"Response\", hue=\"Gender\", kind=\"count\", palette=\"pastel\", data=insurance_df)\n",
        "\n",
        "# Adding x and y labels with font size specifications\n",
        "plt.xlabel('Response', fontdict={'fontsize':12})\n",
        "plt.ylabel('Count', fontdict={'fontsize':14})\n",
        "\n",
        "# Adding a title with font size and weight specifications\n",
        "plt.title('Response V/S Gender', fontdict={'fontsize':15, 'fontweight':'bold'})\n",
        "\n",
        "# Displaying the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I picked the catplot with a count type and hue as \"Gender\" because it effectively shows the distribution of the \"Response\" variable by gender. The count type shows the frequency of responses in each category, and using \"Gender\" as the hue allows us to compare the response rate between male and female customers.\n",
        "\n",
        "This type of visualization can be useful for understanding any potential differences in customer response rates based on gender and can help inform marketing strategies and target customer outreach."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The catplot with a count type and hue as \"Gender\" shows that in both Response categories, i.e., \"0\" and \"1\", male customers have a higher count than female customers. This suggests that male customers may be more responsive to the company's outreach efforts. However, further analysis may be needed to determine whether this trend holds across different age groups or regions, and to identify any potential underlying factors influencing customer response rates."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights gained from data analysis can be useful in creating a positive business impact by helping companies identify customer segments that are more responsive to their products and services, and by informing targeted marketing strategies and customer outreach efforts. For example, the analysis of the \"Response\" variable by age group, region, and policy sales channel can help the company better understand customer preferences and tailor their marketing and outreach efforts accordingly.\n",
        "\n",
        "However, there could be potential negative impacts from the analysis if the company uses the insights in a way that is discriminatory or violates customer privacy. For example, if the company were to use demographic information such as age or gender to target or exclude certain groups of customers, this could be seen as discriminatory and potentially harmful to the business."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Analysis of Age Group and Previously Insured.**"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    #Create a figure with two subplots\n",
        "    fig, axes = plt.subplots(1,2, figsize=(25,8))\n",
        "\n",
        "    #Create a count plot on the first subplot\n",
        "    sns.countplot(ax = axes[0],x=\"Response\", hue=\"Age_Group\", palette=\"pastel\",\n",
        "            data=insurance_df)\n",
        "\n",
        "    #Set the x and y labels and title for the first subplot\n",
        "    axes[0].set_xlabel(xlabel = 'Response', fontdict={'fontsize': 14})\n",
        "    axes[0].set_ylabel(ylabel = 'Count', fontdict={'fontsize': 14})\n",
        "    axes[0].set_title('Age_Group', fontdict={'fontsize': 15, 'fontweight':'bold'})\n",
        "\n",
        "    #Create a histogram plot on the second subplot\n",
        "    sns.histplot(ax = axes[1],binwidth=0.5, x=\"Age_Group\",\n",
        "                 hue=\"Previously_Insured\", data=insurance_df,\n",
        "                 stat=\"count\", multiple=\"stack\")\n",
        "\n",
        "    #Set the x and y labels and title for the second subplot\n",
        "    axes[1].set_xlabel(xlabel = 'Age_Group', fontdict={'fontsize': 14})\n",
        "    axes[1].set_ylabel(ylabel = 'Count', fontdict={'fontsize': 14})\n",
        "    axes[1].set_title('Age_Group V/S Previously_Insured', fontdict={'fontsize': 15, 'fontweight':'bold'})"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first chart is a count plot, which is used to show the distribution of categorical variables. It shows the count of the Response variable based on the Age_Group variable using different colors for each age group. This chart is useful to get an idea of the number of people who responded to the insurance offer based on their age group.\n",
        "\n",
        "The second chart is a histogram plot that shows the distribution of a numeric variable, Age_Group, with different colors for people who are Previously_Insured or not. This plot is useful to show the relationship between Age_Group and Previously_Insured variables, and how they are distributed in the data.\n",
        "\n",
        "In summary, the specific charts were chosen based on the variables of interest and the best way to visualize them."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First plot: In response 0, both young adults and middle-aged adults have high counts, while in response 1, the count for middle-aged adults is the highest among all age groups.\n",
        "\n",
        "Second plot: The ratio of young adults who are previously insured to those who are not previously insured is highest, as compared to middle-aged and senior adults, indicating that young adults are more likely to have prior insurance coverage."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "in the first plot, the fact that middle-aged adults have the highest count in response 1 could be a valuable insight for a business that is trying to target its marketing efforts towards a specific age group. The business may decide to focus more on middle-aged adults in their advertising campaigns to capitalize on this finding.\n",
        "\n",
        "On the other hand, there may be insights that could lead to negative growth. For example, if the second plot shows that young adults who are previously insured are more likely to switch to a different insurance provider, it could lead to negative growth for the current insurance provider. This could be because the current provider is not meeting the needs of these young adults, or because a competitor is offering a more attractive option."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Vehicle_Damage V/S Response and Annual_Premium**"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create subplots for two plots side by side\n",
        "fig, axes = plt.subplots(1, 2, figsize=(22, 8))\n",
        "\n",
        "# First plot - Point plot for Vehicle_Damage vs Response\n",
        "sns.pointplot(ax=axes[0], x=\"Vehicle_Damage\", y=\"Response\", hue=\"Vehicle_Age\", aspect=0.7, kind=\"point\", data=insurance_df)\n",
        "\n",
        "# Set x and y labels and title for first plot\n",
        "axes[0].set_xlabel(xlabel='Vehicle_Damage', fontdict={'fontsize': 14})\n",
        "axes[0].set_ylabel(ylabel='Response', fontdict={'fontsize': 14})\n",
        "axes[0].set_title('Vehicle_Damage V/S Response', fontdict={'fontsize': 15, 'fontweight': 'bold'})\n",
        "\n",
        "# Second plot - Point plot for Vehicle_Damage vs Annual_Premium\n",
        "sns.pointplot(x='Vehicle_Damage', y='Annual_Premium', data=insurance_df, kind='point', ax=axes[1])\n",
        "\n",
        "# Set x and y labels and title for second plot\n",
        "axes[1].set_xlabel(xlabel='Vehicle_Damage', fontdict={'fontsize': 14})\n",
        "axes[1].set_ylabel(ylabel='Annual_Premium', fontdict={'fontsize': 14})\n",
        "axes[1].set_title('Vehicle_Damage V/S Annual_Premium', fontdict={'fontsize': 15, 'fontweight': 'bold'})\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The specific chart used in the code is a point plot. A point plot is a good choice for visualizing the relationship between two variables when the independent variable is categorical and the dependent variable is numerical.\n",
        "\n",
        "In the first plot, Vehicle_Damage is the independent categorical variable, and Response is the dependent numerical variable. The plot shows the mean of Response for each category of Vehicle_Damage. The hue parameter is used to show the relationship between Vehicle_Damage, Response, and Vehicle_Age by using different colors for each level of Vehicle_Age.\n",
        "\n",
        "In the second plot, Vehicle_Damage is again the independent categorical variable, and Annual_Premium is the dependent numerical variable. The plot shows the mean of Annual_Premium for each category of Vehicle_Damage."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first plot shows that vehicle age is an important factor in determining the response rate to insurance offers after a vehicle has been damaged. The plot indicates that vehicles with an age greater than 2 years have the highest probability of getting a response, followed by vehicles with an age range of 1 to 2 years. This suggests that the age of the vehicle plays a role in determining the customer's willingness to purchase insurance after their vehicle has been damaged.\n",
        "\n",
        "The second plot reveals an interesting relationship between vehicle damage and annual premium. The plot indicates that vehicles which have been damaged pay a higher annual premium than those which have not been damaged. This finding suggests that insurance companies are pricing their policies based on the risk associated with insuring a vehicle that has been previously damaged. Customers with damaged vehicles may be willing to pay a higher premium for coverage due to the increased risk associated with insuring a previously damaged vehicle."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights gained from the plots can potentially create a positive business impact for insurance companies. For example:\n",
        "\n",
        "The first plot suggests that customers are more likely to respond to insurance offers when their vehicle is damaged and when the vehicle is over 2 years old. This insight could be used by insurance companies to target their marketing efforts towards customers with older vehicles or vehicles that have been previously damaged. This could potentially increase the response rate to insurance offers, leading to an increase in business for insurance companies.\n",
        "\n",
        "The second plot suggests that customers with damaged vehicles pay a higher annual premium for insurance. This insight could be used by insurance companies to adjust their pricing strategies and offer higher premiums for customers with damaged vehicles. This could potentially increase revenue for insurance companies."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Vehicle Age with hue Responses**"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Setting the figure size\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "#Countplot of Vehicle age column.\n",
        "sns.countplot(x = 'Vehicle_Age', hue='Response', data = insurance_df)\n",
        "\n",
        "#Setting labels of x and y\n",
        "plt.xlabel(xlabel = 'Vehicle_Age', fontdict={'fontsize': 14})\n",
        "plt.ylabel(ylabel = 'Count', fontdict={'fontsize': 14})\n",
        "plt.title('Vehicle_Age')\n"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The  chart is a count plot, which is used to show the distribution of categorical variables. It shows the count of the Response variable based on the Vehicle_age variable using different colors for each age group. This chart is useful to get an idea of the number of people who responded to the insurance offer based on their age group."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we get the highest response 1 when the vehicle age is 1 to 2 years followed by less than 1 year vehicle age."
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insight that the highest response occurs when the vehicle age is between 1 to 2 years, followed by less than 1 year, can potentially help businesses make more informed decisions about their products and services related to vehicles.\n",
        "\n",
        "For example, businesses that offer vehicle maintenance services can use this insight to target customers with vehicles in this age range. By offering maintenance packages and services that are tailored to the needs of vehicles in this age range, businesses can increase their sales and revenue.\n",
        "\n",
        "Similarly, businesses that sell or lease vehicles can use this insight to inform their inventory and pricing decisions. By offering more vehicles in the 1 to 2-year age range and adjusting pricing accordingly, businesses may be able to attract more customers and increase their sales."
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Vehicle_Age V/S Annual_Premium**"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#SEtting the size of the figure\n",
        "plt.figure(figsize = (15,8))\n",
        "# plotting a box plot\n",
        "sns.boxplot( y = 'Annual_Premium', x = 'Vehicle_Age', hue = 'Vehicle_Damage', data=insurance_df )\n",
        "\n",
        "#Setting the labels and titles\n",
        "plt.xlabel(xlabel = 'Vehicle_Age')\n",
        "plt.ylabel(ylabel = 'Annual_Premium')\n",
        "plt.title('Vehicle_Age V/S Annual_Premium')"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Box and whisker plots are very effective and easy to read, as they can summarize data from multiple sources and display the results in a single graph. Box and whisker plots allow for comparison of data from different categories for easier, more effective decision-making.\n",
        "\n",
        "Box plots are used to show distributions of numeric data values, especially when you want to compare them between multiple groups. They are built to provide high-level information at a glance, offering general information about a group of data's symmetry, skew, variance, and outliers."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vehicles older than two years appear to be subject to higher premiums than the others."
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insight that vehicles older than two years appear to be subject to higher premiums than others can potentially help insurance companies adjust their pricing strategies and better target their customer segments.\n",
        "\n",
        "For example, insurance companies can use this insight to adjust their pricing models for vehicles based on their age. By offering lower premiums for newer vehicles and higher premiums for older vehicles, insurance companies can potentially attract more customers and increase their revenue.\n",
        "\n",
        "Additionally, insurance companies can use this insight to better target their customer segments. By identifying customers who own vehicles that are older than two years and offering them specific policies and services that are tailored to their needs, insurance companies can increase customer loyalty and retention."
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Analysis of Annual_Premium V/S Age_Group**"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Setting the size of figure\n",
        "plt.figure(figsize = (15,8))\n",
        "\n",
        "#plotting a bar chart\n",
        "sns.barplot(y = 'Annual_Premium', x = 'Age_Group', data= insurance_df)\n",
        "\n",
        "#Setting the lables and tiles\n",
        "plt.xlabel(xlabel = 'Age_Group')\n",
        "plt.ylabel(ylabel = 'Annual_Premium_Treated')\n",
        "plt.title('Annual_Premium V/S Age_Group')"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar charts enable us to compare numerical values like integers and percentages. They use the length of each bar to represent the value of each variable. For example, bar charts show variations in categories or subcategories scaling width or height across simple, spaced bars, or rectangles."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Senior people seems to paying anual premium stilghtly higher than Young Adults and MiddleAdults."
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Analysis of Policy_Sales_Channel**"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create subplots for six plots side by side\n",
        "fig, axes = plt.subplots(2,3, figsize=(22,15))\n",
        "\n",
        "#Creating a bar chart between Policy_Sales_Channel_Categorical and Vintage\n",
        "sns.barplot(ax = axes[0][0], x='Policy_Sales_Channel_Categorical', y='Vintage',data=insurance_df)\n",
        "axes[0][0].set_xlabel(xlabel = 'Policy_Sales_Channel_Categorical', fontdict={'fontsize': 14})\n",
        "axes[0][0].set_ylabel(ylabel = 'Vintage', fontdict={'fontsize': 14})\n",
        "axes[0][0].set_title('Policy_Sales_Channel V/S Vintage',\n",
        "                    fontdict={'fontsize': 15, 'fontweight':'bold'})\n",
        "\n",
        "#Creating a bar chart between Policy_Sales_Channel_Categorical and Annual_Premium\n",
        "sns.barplot(ax = axes[0][1], x='Policy_Sales_Channel_Categorical', y='Annual_Premium',data=insurance_df)\n",
        "axes[0][1].set_xlabel(xlabel = 'Policy_Sales_Channel_Categorical', fontdict={'fontsize': 14})\n",
        "axes[0][1].set_ylabel(ylabel = 'Annual_Premium', fontdict={'fontsize': 14})\n",
        "axes[0][1].set_title('Policy_Sales_Channel V/S Annual_Premium',\n",
        "                    fontdict={'fontsize': 15, 'fontweight':'bold'})\n",
        "\n",
        "#Creating a horizontal bar chart counting Policy_Sales_Channel_Categorical\n",
        "insurance_df['Policy_Sales_Channel_Categorical'].value_counts().plot(ax = axes[0][2] ,kind='barh')\n",
        "axes[0][2].set_xlabel(xlabel = 'Count', fontdict={'fontsize': 14})\n",
        "axes[0][2].set_ylabel(ylabel = 'Policy_Sales_Channel_Categorical', fontdict={'fontsize': 14})\n",
        "axes[0][2].set_title('Policy_Sales_Channel', fontdict={'fontsize': 15, 'fontweight':'bold'})\n",
        "\n",
        "#plotting histogram for Policy_Sales_Channel_Categorical with hue Response\n",
        "sns.histplot(ax = axes[1][0],x=\"Policy_Sales_Channel_Categorical\", hue=\"Response\", data=insurance_df, stat=\"count\",\n",
        "            multiple=\"stack\",binwidth=0.5)\n",
        "axes[1][0].set_xlabel(xlabel = 'Policy_Sales_Channel_Categorical', fontdict={'fontsize': 14})\n",
        "axes[1][0].set_ylabel(ylabel = 'Count', fontdict={'fontsize': 14})\n",
        "axes[1][0].set_title('Policy_Sales_Channel', fontdict={'fontsize': 15, 'fontweight':'bold'})\n",
        "\n",
        "#Grouping Policy_Sales_Channel_Categorical and summing them\n",
        "groupPolicySalesBySum=insurance_df.groupby(by=[\"Policy_Sales_Channel_Categorical\"]).sum().reset_index()\n",
        "\n",
        "#Plotting the barchart for the grouped Policy_Sales_Channel_Categorical\n",
        "sns.barplot(ax = axes[1][1], x=\"Policy_Sales_Channel_Categorical\", y=\"Response\", data=groupPolicySalesBySum)\n",
        "axes[1][1].set_xlabel(xlabel = 'Policy_Sales_Channel_Categorical', fontdict={'fontsize': 14})\n",
        "axes[1][1].set_ylabel(ylabel = 'Response', fontdict={'fontsize': 14})\n",
        "axes[1][1].set_title('Policy_Sales_Channel V/S Response', fontdict={'fontsize': 15, 'fontweight':'bold'})\n",
        "\n",
        "#Plotting the barchart betweeen Policy_Sales_Channel_Categorical and REsponse with hue\n",
        "sns.barplot(ax = axes[1][2], x='Policy_Sales_Channel_Categorical', y='Response', data=insurance_df, hue='Region_Code_Categorical')\n",
        "axes[1][2].set_xlabel(xlabel = 'Policy_Sales_Channel_Categorical', fontdict={'fontsize': 14})\n",
        "axes[1][2].set_ylabel(ylabel = 'Response', fontdict={'fontsize': 14})\n",
        "axes[1][2].set_title('Policy_Sales_Channel V/S Response', fontdict={'fontsize': 15, 'fontweight':'bold'})\n"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar charts enable us to compare numerical values like integers and percentages. They use the length of each bar to represent the value of each variable. For example, bar charts show variations in categories or subcategories scaling width or height across simple, spaced bars, or rectangles..\n",
        "Here it is used for Policy_Sales_Channel_Categorical variable ."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The average Annual Premium is highest for the Channel B .\n",
        "\n",
        "Channel D has the highest count followed by Channel A."
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights that the average Annual Premium is highest for Channel B, and that Channel D has the highest count followed by Channel A, can potentially help businesses make more informed decisions about their sales channels and pricing strategies.\n",
        "\n",
        "For example, if a business is looking to optimize their sales channels to increase their revenue, the insight that Channel D has the highest count followed by Channel A can help guide their decision-making process. By investing more resources into these channels and tailoring their marketing efforts to appeal to their customers, businesses can potentially increase their sales and revenue.\n",
        "\n",
        "Similarly, if a business is looking to adjust their pricing strategies to optimize their revenue, the insight that the average Annual Premium is highest for Channel B can be very useful. By offering products and services with higher premiums through this channel, businesses can potentially increase their revenue and profitability."
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Analysis of Responses over numerical variables**"
      ],
      "metadata": {
        "id": "UPxgebpLxKsT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Storing only categorical columns\n",
        "categorical_columns = ['Gender', 'Age_Group', 'Region_Code_Categorical', 'Previously_Insured', 'Vehicle_Age','Vehicle_Damage', 'Policy_Sales_Channel_Categorical']\n",
        "\n",
        "#Setting the 14 sub countplots side by side\n",
        "fig, axes =  plt.subplots(2, 7, figsize=(45, 15))\n",
        "\n",
        "for i in range(7):\n",
        "    #Countplot of Response 1 for all the categorical columns.\n",
        "    sns.countplot(data = insurance_df[insurance_df['Response']==1], x=categorical_columns[i], ax=axes[0][i])\n",
        "    axes[0][i].set_xlabel(xlabel = categorical_columns[i], fontdict={'fontsize': 14})\n",
        "    axes[0][i].set_ylabel(ylabel = 'Count', fontdict={'fontsize': 14})\n",
        "    axes[0][i].set_title(categorical_columns[i],\n",
        "                      fontdict={'fontsize': 15, 'fontweight':'bold'})\n",
        "\n",
        "    #Countplot of Response 0 for all the categorical columns.\n",
        "    sns.countplot(data = insurance_df[insurance_df['Response']==0], x=categorical_columns[i], ax=axes[1][i])\n",
        "\n",
        "    axes[1][i].set_xlabel(xlabel = categorical_columns[i], fontdict={'fontsize': 14})\n",
        "    axes[1][i].set_ylabel(ylabel = 'Count', fontdict={'fontsize': 14})\n",
        "    axes[1][i].set_title(categorical_columns[i],\n",
        "                      fontdict={'fontsize': 15, 'fontweight':'bold'})\n"
      ],
      "metadata": {
        "id": "n9pArQLXjVzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "s-B7NoA_dKp4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "countplot() function to visualize data in the deep-learning or statistical investigation using the seaborn countplot. The countplot is primarily used to display observational counts in different category-based bins using bars.\n",
        "Show the counts of observations in each categorical bin using bars. A count plot can be thought of as a histogram across a categorical, instead of quantitative, variable. The basic API and options are identical to those for barplot() , so you can compare counts across nested variables."
      ],
      "metadata": {
        "id": "lr_H_ZtddKp4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "VbKvD0c5dPWk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first row we have count plot of numeric variables for Response 1 ,\n",
        "highest counts = Gender : Male, Age_Group : MiddleAdult, Region_code_categorical :  region c , previously_insured : 0,vehicle_Age : 1-2 years, vehicle_Damage : Yes ,Policy_Sales_Channel_Categorical : Channel D.\n",
        "\n",
        "The second row we have count plot of numeric variables for Response 0 ,\n",
        "highest counts = Gender : Male, Age_Group : MiddleAdult and Youngadult,Region_code_categorical :  region c , previously_insured : 1,vehicle_Age : 1-2 years, vehicle_Damage : Yes ,Policy_Sales_Channel_Categorical : Channel D.\n"
      ],
      "metadata": {
        "id": "VDrcATkjdPWk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "eH3ucrwYdVFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights provided by these count plots can potentially help businesses make more informed decisions about their marketing strategies and target customer segments.\n",
        "\n",
        "For example, the insight that the highest counts for Response 1 are Male, MiddleAdult, Region_C, and previously uninsured customers can help businesses tailor their marketing campaigns to appeal to this demographic. By offering targeted promotions and messaging that speaks to the specific needs and concerns of this demographic, businesses can increase their likelihood of success in attracting and retaining these customers.\n",
        "\n",
        "Similarly, the insight that the highest counts for Response 0 are Male, MiddleAdult, and Youngadult, Region_C, previously insured customers, and customers with vehicle ages between 1-2 years can help businesses identify areas where they may be losing potential customers. By analyzing the reasons why these customers are not responding to their marketing efforts, businesses can adjust their strategies and messaging to better meet their needs and increase their likelihood of success in converting these customers."
      ],
      "metadata": {
        "id": "d0L6X82ydWwm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Correlation Heatmap**"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = insurance_df.copy()\n",
        "df.columns\n"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop unnecessary columns\n",
        "df = df.drop(['id', 'Driving_License', 'Age_Group','Policy_Sales_Channel_Categorical','Region_Code_Categorical'], axis=1)\n",
        "\n",
        "# Create a correlation matrix\n",
        "corr = df.corr()\n",
        "\n",
        "# Generate a heatmap of the correlation matrix\n",
        "plt.figure(figsize=(10,8))\n",
        "sns.heatmap(corr, cmap=\"YlGnBu\", annot=True)\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "RSHI_NbQ-Nx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlation heatmaps can be used to find potential relationships between variables and to understand the strength of these relationships. In addition, correlation plots can be used to identify outliers and to detect linear and nonlinear relationships."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In respect to Response, Age seems to be silghtly positively correalted and Previously_insured column slightly negetively correlated."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  **Pair Plot**"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a pairplot\n",
        "sns.pairplot(df, hue='Response', corner=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pairplot visualization comes handy when you want to go for Exploratory data analysis (“EDA”).\n",
        "\n",
        "Pairplot visualizes given data to find the relationship between them where the variables can be continuous or categorical.\n",
        "Plot pairwise relationships in a data-set.\n",
        "\n",
        "Pairplot is a module of seaborn library which provides a high-level interface for drawing attractive and informative statistical graphics."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We observed the relationship and affect of diffrent variables with each other"
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Overall Insights**"
      ],
      "metadata": {
        "id": "ntHwAYEybf1L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The given statements provide insights into various factors and their correlations in the context of a response to insurance offers. Here is a summary of the key points:\n",
        "\n",
        "Age and Previously_Insured: Age shows a slight positive correlation with response, while the previously_insured column exhibits a slight negative correlation.\n",
        "\n",
        "Numeric variables for Response 1: The highest counts in Response 1 are observed for variables such as Gender (Male), Age_Group (MiddleAdult), Region_code_categorical (region c), previously_insured (0), vehicle_Age (1-2 years), vehicle_Damage (Yes), and Policy_Sales_Channel_Categorical (Channel D).\n",
        "\n",
        "Numeric variables for Response 0: The highest counts in Response 0 are observed for variables such as Gender (Male), Age_Group (MiddleAdult and Youngadult), Region_code_categorical (region c), previously_insured (1), vehicle_Age (1-2 years), vehicle_Damage (Yes), and Policy_Sales_Channel_Categorical (Channel D).\n",
        "\n",
        "Average Annual Premium: Channel B has the highest average annual premium.\n",
        "\n",
        "Policy Sales Channels: Channel D has the highest count, followed by Channel A.\n",
        "\n",
        "Age and Annual Premium: Senior people tend to pay slightly higher annual premiums compared to Young Adults and MiddleAdults.\n",
        "\n",
        "Vehicle Age and Response: Vehicles older than two years have the highest response rate, followed by vehicles with an age range of 1 to 2 years.\n",
        "\n",
        "Vehicle Damage and Premium: Vehicles that have been damaged tend to have higher annual premiums compared to undamaged vehicles, suggesting increased pricing due to the associated risk.\n",
        "\n",
        "Gender and Response: Male customers have a higher count than female customers in both response categories, indicating potential responsiveness to outreach efforts.\n",
        "\n",
        "Age Groups and Response: MiddleAdults have the highest count in both response categories, followed by Youngadults and Senior adults.\n",
        "\n",
        "Region Codes: Region_C has the highest count, followed by Region_E.\n",
        "\n",
        "Age and Response: Customers over the age of 40 are more likely to respond to insurance offers.\n",
        "\n",
        "Region Code with Maximum Customers: Region code 28.0 has the highest number of customers (106,415).\n",
        "\n",
        "Mean Age: The mean age for Response 1 is around 42, while for Response 0, it is around 35.\n",
        "\n",
        "Age Group Counts: The counts of the 20-30 age group are the highest for both males and females.\n",
        "\n",
        "This summary provides an overview of the various correlations, patterns, and trends observed in the given statements regarding the response to insurance offers."
      ],
      "metadata": {
        "id": "E2h8fv04bpDQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = data.copy()"
      ],
      "metadata": {
        "id": "2DEtb1eg9rZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Hypothesis Test for Age and Response:\n",
        "\n",
        "Null Hypothesis: There is no significant difference in the mean age of customers who responded to the insurance offer (Response = 1) and those who did not respond (Response = 0).\n",
        "\n",
        "Alternative Hypothesis: The mean age of customers who responded to the insurance offer (Response = 1) is significantly different from those who did not respond (Response = 0)."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import ttest_ind\n",
        "# separate the two groups based on interest in insurance\n",
        "df_response1 = df[df['Response'] == 1]\n",
        "df_response0 = df[df['Response'] == 0]\n",
        "\n",
        "# perform the t-test\n",
        "t_stat, p_value = ttest_ind(df_response1['Age'], df_response0['Age'], equal_var=False)\n",
        "\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"Reject Null Hypothesis: The mean age of customers who responded to the insurance offer is significantly different from those who did not respond.\")\n",
        "else:\n",
        "    print(\"Fail to Reject Null Hypothesis: There is no significant difference in the mean age of customers who responded to the insurance offer and those who did not respond.\")\n"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reject Null Hypothesis: The mean age of customers who responded to the insurance offer is significantly different from those who did not respond."
      ],
      "metadata": {
        "id": "s3a-SCqfPOAe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Therefore, the p-value was obtained from the t-test to test whether there is a significant difference in the mean age between the two groups of customers: those who are interested in purchasing insurance (df_response1['Age']) and those who are not interested (df_response0['Age']).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code is performing an independent two-sample t-test using the ttest_ind() function from the scipy.stats module. The ttest_ind() function returns two values: the t-statistic and the p-value.\n",
        "\n",
        "The t-statistic is a measure of the difference between the means of the two groups, while the p-value represents the probability of obtaining a difference as extreme as the one observed in the sample, assuming that there is no real difference between the two groups in the population (i.e., the null hypothesis is true).\n",
        "\n",
        "If the p-value is less than the chosen significance level (e.g., 0.05), then we reject the null hypothesis and conclude that there is a significant difference in the mean age between the two groups."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hypothesis Test 2: Is there a significant difference in the average annual premium paid by customers who are interested in purchasing insurance versus those who are not interested?\n",
        "\n",
        "Null hypothesis: There is no significant difference in the average annual premium paid by customers who are interested in purchasing insurance versus those who are not interested.\n",
        "\n",
        "Alternative hypothesis: The average annual premium paid by customers who are interested in purchasing insurance is significantly higher than the average annual premium paid by customers who are not interested."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# separate the two groups based on interest in insurance\n",
        "interested = df[df[\"Response\"] == 1][\"Annual_Premium\"]\n",
        "not_interested = df[df[\"Response\"] == 0][\"Annual_Premium\"]\n",
        "\n",
        "# perform the t-test\n",
        "t_stat, p_val = ttest_ind(interested, not_interested)\n",
        "\n",
        "# print the results\n",
        "print(\"t-statistic:\", t_stat)\n",
        "print(\"p-value:\", p_val)\n",
        "\n",
        "alpha = 0.05\n",
        "if p_val < alpha:\n",
        "    print(\"Reject Null Hypothesis: The average annual premium paid by customers who are interested in purchasing insurance is significantly higher than the average annual premium paid by customers who are not interested.\")\n",
        "else:\n",
        "    print(\"Fail to Reject Null Hypothesis: There is no significant difference in the average annual premium paid by customers who are interested in purchasing insurance versus those who are not interested.\")\n"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reject Null Hypothesis: The average annual premium paid by customers who are interested in purchasing insurance is significantly higher than the average annual premium paid by customers who are not interested.\n"
      ],
      "metadata": {
        "id": "UBvR9H9UQGxP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the p-value was obtained from the t-test to test whether there is a significant difference in the mean age between the two groups of customers: those who are interested in purchasing insurance (interested) and those who are not interested (not_interested)."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code is performing an independent two-sample t-test using the ttest_ind() function from the scipy.stats module. The ttest_ind() function returns two values: the t-statistic and the p-value.\n",
        "\n",
        "The t-statistic is a measure of the difference between the means of the two groups, while the p-value represents the probability of obtaining a difference as extreme as the one observed in the sample, assuming that there is no real difference between the two groups in the population (i.e., the null hypothesis is true).\n",
        "\n",
        "If the p-value is less than the chosen significance level (e.g., 0.05), then we reject the null hypothesis and conclude that there is a significant difference in the mean age between the two groups."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hypothesis Test 3: Is there a significant difference in the proportion of customers who have previously purchased insurance between those who are interested in purchasing insurance versus those who are not interested?\n",
        "\n",
        "Null hypothesis: There is no significant difference in the proportion of customers who have previously purchased insurance between those who are interested in purchasing insurance versus those who are not interested.\n",
        "\n",
        "Alternative hypothesis: The proportion of customers who have previously purchased insurance is significantly higher among those who are interested in purchasing insurance compared to those who are not interested."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.stats.proportion import proportions_ztest\n",
        "\n",
        "# separate the two groups based on interest in insurance\n",
        "interested = df[df[\"Response\"] == 1][\"Previously_Insured\"]\n",
        "not_interested = df[df[\"Response\"] == 0][\"Previously_Insured\"]\n",
        "\n",
        "# perform the z-test\n",
        "count = [interested.sum(), not_interested.sum()]\n",
        "nobs = [len(interested), len(not_interested)]\n",
        "z_stat, p_val = proportions_ztest(count, nobs)\n",
        "\n",
        "# print the results\n",
        "print(\"z-statistic:\", z_stat)\n",
        "print(\"p-value:\", p_val)\n",
        "\n",
        "alpha = 0.05\n",
        "if p_val < alpha:\n",
        "    print(\"Reject Null Hypothesis: The proportion of customers who have previously purchased insurance is significantly higher among those who are interested in purchasing insurance compared to those who are not interested.\")\n",
        "else:\n",
        "    print(\"Fail to Reject Null Hypothesis: There is no significant difference in the proportion of customers who have previously purchased insurance between those who are interested in purchasing insurance versus those who are not interested.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reject Null Hypothesis: The proportion of customers who have previously purchased insurance is significantly higher among those who are interested in purchasing insurance compared to those who are not interested.\n"
      ],
      "metadata": {
        "id": "cG-s-n0lQJv-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " the p-value was obtained from the z-test to test whether there is a significant difference in the proportion of customers interested in purchasing insurance (interested.sum()) versus those who are not interested (not_interested.sum()) in the population."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code is performing a two-proportions z-test using the proportions_ztest() function from the statsmodels.stats.proportion module. The proportions_ztest() function returns two values: the z-statistic and the p-value.\n",
        "\n",
        "The z-statistic is a measure of the difference between the proportions of the two groups, while the p-value represents the probability of obtaining a difference as extreme as the one observed in the sample, assuming that there is no real difference between the two groups in the population (i.e., the null hypothesis is true).\n",
        "\n",
        "The count parameter specifies the number of successes in each group, while the nobs parameter specifies the total number of trials in each group.\n",
        "\n",
        "If the p-value is less than the chosen significance level (e.g., 0.05), then we reject the null hypothesis and conclude that there is a significant difference in the proportion of customers interested in purchasing insurance between the two groups."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No missing or null values found."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "fig, axes = plt.subplots(1, 3, figsize=(20,5))\n",
        "\n",
        "sns.boxplot(ax = axes[0],y = 'Annual_Premium',x = 'Response', data = df)\n",
        "axes[0].set_xlabel(xlabel = 'Response')\n",
        "axes[0].set_ylabel(ylabel = 'Annual_Premium')\n",
        "axes[0].set_title('Annual_Premium')\n",
        "\n",
        "sns.boxplot(ax = axes[1],y = 'Age',x = 'Response', data = df)\n",
        "axes[1].set_xlabel(xlabel = 'Response')\n",
        "axes[1].set_ylabel(ylabel = 'Age')\n",
        "axes[1].set_title('Age')\n",
        "\n",
        "sns.boxplot(ax = axes[2],y = 'Vintage',x = 'Response', data = df)\n",
        "axes[2].set_xlabel(xlabel = 'Response')\n",
        "axes[2].set_ylabel(ylabel = 'Vintage')\n",
        "axes[2].set_title('Vintage')\n",
        "\n",
        "plt.suptitle('Outliers', fontsize = 22, fontweight = 'bold' )"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**INSIGHTS**"
      ],
      "metadata": {
        "id": "Z3ukzjk2SfN9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The plotted data suggests that the Annual Premium distribution is skewed towards positive values. Additionally, it can be observed that the Vintage variable has no such outliers. Although the Age column contains some outliers, we have decided not to address them as they will not impact our results."
      ],
      "metadata": {
        "id": "bcOXVpWrS73d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#25 percentile quatile\n",
        "Q1=df['Annual_Premium'].quantile(0.25)\n",
        "\n",
        "#75 percentile quatile\n",
        "Q3=df['Annual_Premium'].quantile(0.75)\n",
        "\n",
        "#Inter quatile range\n",
        "IQR=Q3-Q1\n",
        "\n",
        "Lower_Whisker = Q1-1.5*IQR\n",
        "Upper_Whisker = Q3+1.5*IQR\n",
        "\n",
        "#Removing Outliers\n",
        "df['Annual_Premium_Treated'] = np.where(df['Annual_Premium']>Upper_Whisker, Upper_Whisker, df['Annual_Premium'])"
      ],
      "metadata": {
        "id": "KFUmN6D8TUnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Setting figure size\n",
        "plt.figure(figsize=(10,5))\n",
        "\n",
        "#Plotting a box plot\n",
        "sns.boxplot( y = 'Annual_Premium_Treated',x = 'Response', data = df)\n",
        "\n",
        "#Labels and titles\n",
        "plt.xlabel(xlabel = 'Response')\n",
        "plt.ylabel(ylabel = 'Annual_Premium_Treated')\n",
        "plt.title('Annual Premium Treated')\n"
      ],
      "metadata": {
        "id": "0fJDsBO5Ty1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**INSIGHTS**"
      ],
      "metadata": {
        "id": "WgOEtLdvUpq4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above plots we can see that there are no more outliers in Annual Premium."
      ],
      "metadata": {
        "id": "qfuEU38LVd5J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The outlier treatment technique used in the code snippet is called the \"IQR method\" or \"Tukey's method\". It involves calculating the interquartile range (IQR), which is the difference between the 75th percentile (Q3) and the 25th percentile (Q1) of the data. Then, a lower whisker is calculated as Q1 - 1.5IQR, and an upper whisker is calculated as Q3 + 1.5IQR. Any data points that fall outside of these whiskers are considered outliers and are either removed or replaced.\n",
        "\n",
        "In this code, the upper whisker is used to replace any values in the 'Annual_Premium' column that are greater than it, effectively capping the values at the upper limit of what is considered normal or expected. The reason for choosing this technique could be that it is a relatively simple and commonly used method for detecting and treating outliers. Additionally, this method is less likely to remove too many data points compared to other outlier treatment techniques, which can result in biased or incomplete analysis."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "df.nunique()"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Categorizing Policy_Sales_Channel feature\n",
        "channel_bins = [0, 41, 81, 121,165]\n",
        "channel_labels = ['Channel_A','Channel_B', 'Channel_C', 'Channel_D']\n",
        "df['Policy_Sales_Channel_Categorical'] = pd.cut(df['Policy_Sales_Channel'], bins=channel_bins, labels=channel_labels)\n",
        "\n",
        "# Categorizing Region_Code feature\n",
        "region_bins = [0,11,21,31,41,53]\n",
        "region_labels = ['Region_E','Region_D','Region_C', 'Region_B', 'Region_A']\n",
        "df['Region_Code_Categorical'] = pd.cut(df['Region_Code'], bins=region_bins, labels=region_labels)"
      ],
      "metadata": {
        "id": "2UHkjYaoYCZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.nunique()"
      ],
      "metadata": {
        "id": "l5i1g7TDYagW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_features = ['Vehicle_Age','Policy_Sales_Channel_Categorical','Region_Code_Categorical']"
      ],
      "metadata": {
        "id": "b76ldmb0YhxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(labels = ['id','Driving_License','Region_Code','Policy_Sales_Channel','Annual_Premium'], axis = 1)"
      ],
      "metadata": {
        "id": "gEPIaRIrY4W_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "QnInsdFgg-MG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Assigning Male to 1 and Female to 0\n",
        "df['Gender'] = df['Gender'].apply(lambda x : 1 if x == 'male' else 0)\n",
        "#Assigning Yes to 1 and No to 0\n",
        "df['Vehicle_Damage'] = df['Vehicle_Damage'].apply(lambda x : 1 if x == \"Yes\" else 0)"
      ],
      "metadata": {
        "id": "HK5VAqP0hW6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encode the categorical features using get_dummies\n",
        "df_encoded = pd.get_dummies(df, columns=categorical_features)\n"
      ],
      "metadata": {
        "id": "p02--WAUaoOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_encoded.info()"
      ],
      "metadata": {
        "id": "IkfpRyZAhkOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df_encoded"
      ],
      "metadata": {
        "id": "2SwahHZsa-MO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One hot encoding is a technique used in machine learning to transform categorical data into a numerical representation that can be used in predictive models. In one hot encoding, each category is represented as a binary vector, where each element in the vector corresponds to a category and is either 0 or 1 depending on whether the sample belongs to that category or not. One hot encoding is used because many machine learning algorithms can't handle categorical data directly, and require numerical data instead. By converting categorical data into a numerical form using one hot encoding, we can make use of a wider range of machine learning algorithms and improve the accuracy of our models. Additionally, one hot encoding is useful because it does not impose any ordinal relationship between categories, which can be a limitation of other encoding techniques. However, one potential downside of one hot encoding is that it can result in a large number of features, which can increase the dimensionality of the data and slow down training times."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.stats.outliers_influence import variance_inflation_factor"
      ],
      "metadata": {
        "id": "7c-q_gp1fcnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "def get_vif_factors(X):\n",
        "  #converting to dataframe to a matrix\n",
        "  X_matrix = X.to_numpy()\n",
        "  #Using list comprehension to store vif of each variable\n",
        "  vif = [ variance_inflation_factor(X_matrix,i) for i in range(X.shape[1])]\n",
        "  #Creating an empty dataframe.\n",
        "  vif_factors = pd.DataFrame()\n",
        "  #Storing columns name\n",
        "  vif_factors['column'] = X.columns\n",
        "  #Storing corresponding vifs\n",
        "  vif_factors['vif'] = vif\n",
        "\n",
        "  return vif_factors"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = ['Age','Annual_Premium_Treated','Vintage']"
      ],
      "metadata": {
        "id": "Fg0Y4Z2be_Bd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calling get_vif_factors function\n",
        "vif_factors = get_vif_factors(df[index])\n",
        "vif_factors\n",
        ""
      ],
      "metadata": {
        "id": "DJa8Htd7fNWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (12,10))\n",
        "sns.heatmap(df[index].corr(),annot = True)\n",
        "plt.title(\" Heatmap depicting correlation between features\")\n",
        ""
      ],
      "metadata": {
        "id": "2xOfmhZdfkAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "to improve the accuracy of a machine learning model, it is important to identify any predictor variables that may not have a significant impact on the target variable, which in your case is the 'Sales' column. One way to do this is by examining the model summary and identifying variables with high p-values.\n",
        "\n",
        "Another important consideration is multicollinearity, which occurs when predictor variables are highly correlated with one another. To check for multicollinearity, we calculated the Variance Inflation Factor (VIF) for each variable. A high VIF value, generally considered to be greater than 4, suggests that a variable may be contributing to multicollinearity. To investigate these variables further, we created a heatmap to visualize their relationships with other variables in the model. This helped us gain a better understanding of which variables may need to be modified or removed to improve model performance."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will move forward with all the features available as variable are independent with each other means no multicollinearity detected."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is generally not necessary to perform data transformation on the target variable in a binary classification problem, as the target variable only has two possible values (1 or 0) and is already in a numerical format. Data transformation is typically used to convert non-numerical or continuous data into a numerical format that can be processed by machine learning algorithms.\n",
        "\n",
        "In the case of a binary classification problem, the target variable is already in a format that can be used directly by classification algorithms. Transforming the target variable in this case may not provide any additional benefit and may even introduce errors or biases into the model.\n",
        "\n",
        "It is important to note that while data transformation may not be necessary for the target variable in a binary classification problem, it may still be necessary for other variables in the dataset. For example, features may need to be scaled, normalized, or encoded to improve the performance of the classification algorithm. It is always important to carefully analyze the data and the problem at hand to determine the most appropriate data preprocessing steps to take."
      ],
      "metadata": {
        "id": "Av8zxG7chC3n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the MinMaxScaler from the preprocessing module of the scikit-learn library\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Using the scaler to transform the 'Annual_Premium_Treated' column of the 'df' DataFrame\n",
        "# Reshaping the data using 'values.reshape(-1, 1)' to ensure the input data is in the right shape for the scaler\n",
        "df['Annual_Premium_Treated'] = scaler.fit_transform(df['Annual_Premium_Treated'].values.reshape(-1,1))\n",
        "\n",
        "# Using the scaler to transform the 'Vintage' column of the 'df' DataFrame\n",
        "# Reshaping the data using 'values.reshape(-1, 1)' to ensure the input data is in the right shape for the scaler\n",
        "df['Vintage_Treated'] = scaler.fit_transform(df['Vintage'].values.reshape(-1,1))"
      ],
      "metadata": {
        "id": "BkqugZGwTYJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(labels = ['Vintage'], axis = 1)"
      ],
      "metadata": {
        "id": "0cK2WYCNxYAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The MinMaxScaler method from the preprocessing module of the scikit-learn library has been used to scale the data.\n",
        "\n",
        "This method scales the data to a specified range, usually between 0 and 1. It works by subtracting the minimum value in the feature column and then dividing by the range (i.e., the maximum value minus the minimum value). This ensures that the smallest value in the column is scaled to 0, the largest value is scaled to 1, and all other values are scaled proportionally in between.\n",
        "\n",
        "MinMaxScaler is a good choice when the distribution of the feature values is unknown or non-Gaussian. It is also appropriate when the data values have meaningful bounds (i.e., a maximum and minimum value), as in this case where the annual premium and vintage columns have a natural minimum and maximum value."
      ],
      "metadata": {
        "id": "xOnAmC3Kih5q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have already performed multicollinearity check and feature selection,we may not need dimensionality reduction. This is because the aim of feature selection is to identify and keep the most informative features, while removing the redundant or irrelevant ones. Multicollinearity check ensures that the features are not highly correlated with each other, which can lead to overfitting and instability of the model. By performing these steps, we have already reduced the dimensionality of the data to a set of non-redundant features, which are the most important in explaining the target variable.\n",
        "\n",
        "Moreover, dimensionality reduction techniques like PCA are usually used when the data has a large number of features that are highly correlated or where there are many features with similar importance. In our case, we have already removed the features that are highly correlated and are left with a set of non-redundant features. Therefore, applying PCA may not provide significant improvement in model performance, and may even result in a loss of interpretability of the model.\n",
        "\n",
        "In summary, we have already performed multicollinearity check and feature selection, we may not need dimensionality reduction as you have already reduced the dimensionality of the data to a set of non-redundant and informative features, which are sufficient for modeling the target variable."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.Response.value_counts()"
      ],
      "metadata": {
        "id": "1a5RXRW1NPnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape\n"
      ],
      "metadata": {
        "id": "Yiyn2thSOI0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset is quite imbalanced.Both the classes are not equally represented.There are only 46710 observations in which customers have suscribed as opposed to 381109 observations where Insurance have not suscribed.In such cases, the model may not be able to learn and may be biased towards the class represented.\n",
        "\n",
        "Even if the model predicts that no customer will suscribe(all zeros), it will have an accuracy more than 80%.This is called Accuracy Paradox.But the objective of building a model here is to identify the customers who will respond to insurance(i.e.,increase the number of True Positives)"
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)\n",
        "#Importing resample from *sklearn.utils* package.\n",
        "from sklearn.utils import resample\n",
        "\n",
        "#Separate the case of yes-Response  and no-Response\n",
        "Response_no = df[df['Response'] == 0]\n",
        "Response_yes = df[df['Response'] == 1]\n",
        "\n",
        "#Upsample the yes-Response cases.\n",
        "df_minority_upsampled = resample(Response_yes,replace = True,n_samples=167199)\n",
        "\n",
        "#Combine majority class with upsampled minority class\n",
        "new_df = pd.concat([Response_no,df_minority_upsampled])"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After,upsampling the new_df contains 334399 cases of\n",
        "response = 0 and 167199 cases of Response = 1 in the ratio of 67:33.Before using the dataset, the examples can be shuffled to make sure they are not in a particular order.sklearn.utils has a method shuffle(),which does the shuffling."
      ],
      "metadata": {
        "id": "pkawLgRDR5RW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import shuffle\n",
        "new_df = shuffle(new_df)"
      ],
      "metadata": {
        "id": "QyTEtjd1Skma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df.columns"
      ],
      "metadata": {
        "id": "VLzMTOdeV1wE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df.head()"
      ],
      "metadata": {
        "id": "dwMr1Alsj5sD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NU3MAUS-FNzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Rename columns to remove square brackets, angle brackets, and square braces\n",
        "new_df.columns = [col.replace('[', '').replace(']', '').replace('<', 'less_than').replace('>', 'greater_than') for col in new_df.columns]\n"
      ],
      "metadata": {
        "id": "CvNtybGxEuGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Assigning list of all column names in the DataFrame\n",
        "X_features = list(new_df.columns)\n",
        "\n",
        "#Remove the response variable from the list\n",
        "X_features.remove('Response')\n",
        "\n",
        "#Storing X\n",
        "X = new_df[X_features]"
      ],
      "metadata": {
        "id": "it4AROVhWJAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y = new_df['Response']"
      ],
      "metadata": {
        "id": "UIAEjckYW8hi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One approach to deal with imbalanced dataset is boostrapping .It involves resampling techniques such as UPSAMPLING.\n",
        "\n",
        "Upsampling: Increase the intances of under-represented minority class by replicating the existing observations in the dataset.Sampling with replacement is used for this purpose and is also called OverSampling."
      ],
      "metadata": {
        "id": "jbTgsWTYZa5i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Splitting"
      ],
      "metadata": {
        "id": "958XjkdhXzbl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "x_train,x_test,y_train,y_test = train_test_split(X,Y,test_size = 0.3,random_state = 42)\n"
      ],
      "metadata": {
        "id": "7_kXcVMRXKVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the given code, the data has been split into training and testing sets using a splitting ratio of 0.2, which means 20% of the data is kept aside for testing and the remaining 80% is used for training the model.\n",
        "\n",
        "The choice of splitting ratio depends on the size of the dataset and the problem at hand. In general, a larger ratio of training to testing data is preferred when the dataset is large, as this allows the model to be trained on a more diverse range of examples and can lead to better performance.\n",
        "\n",
        "However, if the dataset is relatively small, a larger ratio of testing to training data is preferred to ensure that the model is evaluated on a sufficient number of examples and that the evaluation is representative of the generalization performance of the model.\n",
        "\n",
        "In this case, a 20% ratio for testing data has been chosen, which is a common ratio used in many machine learning applications. The choice of 20% allows for a large enough test set to evaluate the model's performance, while still leaving a sufficiently large training set to train the model. The random state of 42 is also chosen to ensure that the data is split in a consistent manner across multiple runs of the code."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1 Implementing Logistic Regression"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "clf = LogisticRegression(fit_intercept=True, max_iter=10000)\n",
        "# Fit the Algorithm\n",
        "clf.fit(x_train, y_train)\n",
        ""
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the coefficients\n",
        "clf.coef_\n",
        ""
      ],
      "metadata": {
        "id": "SMBsnJqAfM8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the intercept value\n",
        "clf.intercept_"
      ],
      "metadata": {
        "id": "oCFabu5IfO2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the model\n",
        "# Get the predicted probabilities\n",
        "train_preds = clf.predict_proba(x_train)\n",
        "test_preds = clf.predict_proba(x_test)\n",
        ""
      ],
      "metadata": {
        "id": "J9eI2BiJfVF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the predicted classes\n",
        "train_class_preds = clf.predict(x_train)\n",
        "test_class_preds = clf.predict(x_test)"
      ],
      "metadata": {
        "id": "WbnkSAjRfaCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the accuracy scores\n",
        "train_accuracy = accuracy_score(train_class_preds,y_train)\n",
        "test_accuracy = accuracy_score(test_class_preds,y_test)\n",
        "\n",
        "print(\"The accuracy on train data is \", train_accuracy)\n",
        "print(\"The accuracy on test data is \", test_accuracy)\n",
        ""
      ],
      "metadata": {
        "id": "F3FlGX2mfea7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Get the confusion matrix for both train and test\n",
        "\n",
        "labels = ['Responded', 'Not_Responded']\n",
        "cm = confusion_matrix(y_train, train_class_preds)\n",
        "print(cm)\n",
        "\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cm, annot=True, ax = ax) #annot=True to annotate cells\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels')\n",
        "ax.set_ylabel('True labels')\n",
        "ax.set_title('Confusion Matrix')\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the confusion matrix for both train and test\n",
        "\n",
        "labels = ['Responded', 'Not_Responded']\n",
        "cm = confusion_matrix(y_test, test_class_preds)\n",
        "print(cm)\n",
        "\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cm, annot=True, ax = ax) #annot=True to annotate cells\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels')\n",
        "ax.set_ylabel('True labels')\n",
        "ax.set_title('Confusion Matrix')\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)"
      ],
      "metadata": {
        "id": "p4Q6V5NsrjNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(metrics.classification_report(train_class_preds, y_train))\n",
        "print(\" \")\n",
        "\n",
        "print(\"roc_auc_score\")\n",
        "print(metrics.roc_auc_score(y_train, train_class_preds))\n",
        ""
      ],
      "metadata": {
        "id": "VtT3yhNEf5Ba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "             precision    recall  f1-score   support\n",
        "\n",
        "           0       0.76      0.86      0.81    208901\n",
        "           1       0.74      0.61      0.67    142217\n",
        "\n",
        "    accuracy                           0.76    351118\n",
        "   macro avg       0.75      0.73      0.74    351118\n",
        "\n",
        "weighted avg       0.76      0.76      0.75    351118\n",
        "\n",
        "\n",
        "roc_auc_score\n",
        "0.7531599787801373"
      ],
      "metadata": {
        "id": "iAaPjLI-lmbW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(metrics.classification_report(test_class_preds, y_test))\n",
        "print(\" \")\n",
        "\n",
        "print(\"roc_auc_score\")\n",
        "print(metrics.roc_auc_score(y_test, test_class_preds))\n",
        ""
      ],
      "metadata": {
        "id": "jGdxT_PHf7nq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "            precision    recall  f1-score   support\n",
        "\n",
        "           0       0.77      0.86      0.81     89896\n",
        "           1       0.74      0.61      0.67     60584\n",
        "\n",
        "    accuracy                           0.76    150480\n",
        "   macro avg       0.75      0.73      0.74    150480\n",
        "\n",
        "weighted avg       0.76      0.76      0.75    150480\n",
        "\n",
        "\n",
        "roc_auc_score\n",
        "0.7530099387215005"
      ],
      "metadata": {
        "id": "RbHYsa6jlhTX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the evaluation metrics and the ROC AUC score provided for both the training and testing datasets, we can make the following observations:\n",
        "\n",
        "The precision, recall, and F1-score for both classes (0 and 1) are relatively balanced, with class 0 having slightly higher values than class 1. This indicates that the model is able to predict both classes with reasonable accuracy.\n",
        "\n",
        "The overall accuracy of the model is 76%, which means that it correctly predicts the target variable in 76% of cases. However, accuracy alone can be misleading, especially when the classes are imbalanced.\n",
        "\n",
        "The ROC AUC score for both the training and testing datasets is around 0.75, which suggests that the model is able to distinguish between the positive and negative classes with moderate accuracy. A score of 0.5 indicates that the model is no better than random, while a score of 1 indicates perfect classification.\n",
        "\n",
        "The weighted average F1-score is slightly lower than the accuracy, which suggests that there is some imbalance in the class distribution. This is further supported by the fact that the macro-average F1-score is lower than the weighted average, indicating that the performance of the model is influenced by the imbalance in the class distribution.\n",
        "\n",
        "Overall, the model seems to be performing reasonably well, with a good balance between precision and recall for both classes. However, further analysis of the data and the model is required to understand the factors influencing its performance and identify areas for improvement."
      ],
      "metadata": {
        "id": "I6VEhIlbmgJV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "model = LogisticRegression(max_iter=10000)\n",
        "solvers = ['lbfgs']\n",
        "penalty = ['10','l2','14','16','20','18']\n",
        "c_values = [1000,100, 10, 1.0, 0.1, 0.01,0.001]\n",
        "# define grid search\n",
        "grid = dict(solver=solvers,penalty=penalty,C=c_values)\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='f1',error_score=0)\n",
        "\n",
        "# Fit the Algorithm\n",
        "grid_result=grid_search.fit(x_train, y_train)\n",
        "\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "\n",
        "\n",
        "# Predict on the model\n",
        "# Get the predicted classes\n",
        "train_class_preds = grid_result.predict(x_train)\n",
        "test_class_preds = grid_result.predict(x_test)\n",
        ""
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Best: 0.672391 using {'C': 100, 'penalty': 'l2', 'solver': 'lbfgs'}"
      ],
      "metadata": {
        "id": "6TFmtFzTh9dy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(metrics.classification_report(train_class_preds, y_train))\n",
        "print(\" \")\n",
        "\n",
        "print(\"roc_auc_score\")\n",
        "print(metrics.roc_auc_score(y_train, train_class_preds))\n",
        ""
      ],
      "metadata": {
        "id": "TyxqJw9am1KU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.76      0.86      0.81    208448\n",
        "           1       0.75      0.61      0.67    142670\n",
        "\n",
        "    accuracy                           0.76    351118\n",
        "   macro avg       0.76      0.74      0.74    351118\n",
        "\n",
        "weighted avg       0.76      0.76      0.75    351118\n",
        "\n",
        "\n",
        "roc_auc_score\n",
        "0.7556186278541133\n"
      ],
      "metadata": {
        "id": "VFUjoZOQh2G5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(metrics.classification_report(test_class_preds, y_test))\n",
        "print(\" \")\n",
        "\n",
        "print(\"roc_auc_score\")\n",
        "print(metrics.roc_auc_score(y_test, test_class_preds))\n",
        ""
      ],
      "metadata": {
        "id": "HRl_kI3Om3su"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.76      0.86      0.81     89269\n",
        "           1       0.75      0.61      0.67     61211\n",
        "\n",
        "    accuracy                           0.76    150480\n",
        "   macro avg       0.75      0.73      0.74    150480\n",
        "\n",
        "weighted avg       0.76      0.76      0.75    150480\n",
        "\n",
        "\n",
        "roc_auc_score\n",
        "0.7536922958734591\n"
      ],
      "metadata": {
        "id": "UgYoQAuDiLXi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearchCV which uses the Grid Search technique for finding the optimal hyperparameters to increase the model performance.\n",
        "\n",
        "our goal should be to find the best hyperparameters values to get the perfect prediction results from our model. But the question arises, how to find these best sets of hyperparameters? One can try the Manual Search method, by using the hit and trial process and can find the best hyperparameters which would take huge time to build a single model.\n",
        "\n",
        "For this reason, methods like Random Search, GridSearch were introduced. Grid Search uses a different combination of all the specified hyperparameters and their values and calculates the performance for each combination and selects the best value for the hyperparameters. This makes the processing time-consuming and expensive based on the number of hyperparameters involved.\n",
        "\n",
        "In GridSearchCV, along with Grid Search, cross-validation is also performed. Cross-Validation is used while training the model.\n",
        "\n",
        "That's why I have used GridsearCV method for hyperparameter optimization."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the provided information, it seems that there is no significant improvement in the performance of the logistic regression model after hyperparameter tuning. Although the best hyperparameters were selected using grid search, the resulting precision, recall, f1-score, and AUC scores in the test set are very similar to those obtained with the default hyperparameters.\n",
        "\n",
        "However, it's worth noting that the model's performance is quite good, with an accuracy, precision, recall, and f1-score above 0.75 for both the train and test sets. The AUC score is also reasonably high, indicating that the model can discriminate well between positive and negative cases.\n",
        "\n",
        "Overall, while there may not have been a significant improvement in the model's performance, the logistic regression model seems to be a reliable and effective classifier for the given dataset."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2 RandomForestClassifier"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 2 Implementation\n",
        "# Create an instance of the RandomForestClassifier\n",
        "rf_model = RandomForestClassifier()\n",
        "\n",
        "# Fit the Algorithm\n",
        "rf_model.fit(x_train,y_train)\n",
        "\n",
        "# Predict on the model\n",
        "# Making predictions on train and test data\n",
        "train_class_preds = rf_model.predict(x_train)\n",
        "test_class_preds = rf_model.predict(x_test)"
      ],
      "metadata": {
        "id": "5cYyGmn1qlZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating accuracy on train and test\n",
        "train_accuracy = accuracy_score(y_train,train_class_preds)\n",
        "test_accuracy = accuracy_score(y_test,test_class_preds)\n",
        "\n",
        "print(\"The accuracy on train dataset is\", train_accuracy)\n",
        "print(\"The accuracy on test dataset is\", test_accuracy)\n",
        ""
      ],
      "metadata": {
        "id": "OFvxidkqqn6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The accuracy on train dataset is 0.997052273025023\n",
        "\n",
        "The accuracy on test dataset is 0.9109316852737905"
      ],
      "metadata": {
        "id": "Ck_2jpsgn_rz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart# Get the confusion matrix for both train and test\n",
        "\n",
        "labels = ['Rsponded', 'Not_Responded']\n",
        "cm = confusion_matrix(y_train, train_class_preds)\n",
        "print(cm)\n",
        "\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cm, annot=True, ax = ax) #annot=True to annotate cells\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels')\n",
        "ax.set_ylabel('True labels')\n",
        "ax.set_title('Confusion Matrix')\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the confusion matrix for both train and test\n",
        "\n",
        "labels = ['Rsponded', 'Not_Responded']\n",
        "cm = confusion_matrix(y_test, test_class_preds)\n",
        "print(cm)\n",
        "\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cm, annot=True, ax = ax) #annot=True to annotate cells\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels')\n",
        "ax.set_ylabel('True labels')\n",
        "ax.set_title('Confusion Matrix')\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)"
      ],
      "metadata": {
        "id": "e6fLf54KqwVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(metrics.classification_report(train_class_preds, y_train))\n",
        "print(\" \")\n",
        "\n",
        "print(\"roc_auc_score\")\n",
        "print(metrics.roc_auc_score(y_train, train_class_preds))"
      ],
      "metadata": {
        "id": "yY32WjCTqygn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       1.00      1.00      1.00    233127\n",
        "           1       1.00      0.99      1.00    117991\n",
        "\n",
        "    accuracy                           1.00    351118\n",
        "   macro avg       1.00      1.00      1.00    351118\n",
        "\n",
        "weighted avg       1.00      1.00      1.00    351118\n",
        "\n",
        "\n",
        "roc_auc_score\n",
        "0.9974447975740884"
      ],
      "metadata": {
        "id": "yigkKiBuoG36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(metrics.classification_report(test_class_preds, y_test))\n",
        "print(\" \")\n",
        "\n",
        "print(\"roc_auc_score\")\n",
        "print(metrics.roc_auc_score(y_test, test_class_preds))"
      ],
      "metadata": {
        "id": "vqtmcjgPq0ak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "             precision    recall  f1-score   support\n",
        "\n",
        "           0       0.90      0.97      0.93     93042\n",
        "           1       0.94      0.82      0.88     57438\n",
        "\n",
        "    accuracy                           0.91    150480\n",
        "   macro avg       0.92      0.89      0.90    150480\n",
        "\n",
        "weighted avg       0.91      0.91      0.91    150480\n",
        "\n",
        "\n",
        "roc_auc_score\n",
        "0.9185141588238699"
      ],
      "metadata": {
        "id": "1iY5hF34oNqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It looks like our model is performing very well on both the training and test sets. The precision, recall, and F1-score are all very high, indicating that the model is accurately identifying both positive and negative cases. Additionally, the AUC-ROC score is also quite high, which means that the model is effectively separating the positive and negative classes.\n",
        "\n",
        "However, it's worth noting that perfect performance on the training set doesn't necessarily guarantee that the model will perform well on unseen data. It's possible that the model is overfitting to the training data and may not generalize well to new data. Therefore, it's important to monitor performance on the test set to ensure that the model is not overfitting.\n",
        "\n",
        "Overall, our model's performance on the test set is quite good, with high precision, recall, and F1-score, as well as a high AUC-ROC score. This suggests that the model is likely to perform well on new, unseen data."
      ],
      "metadata": {
        "id": "lPkQRbNnDn2g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 2 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "# Number of trees\n",
        "n_estimators = [50,80,100]\n",
        "\n",
        "# Maximum depth of trees\n",
        "max_depth = [4,6,8]\n",
        "\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [50,100,150]\n",
        "\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [40,50]\n",
        "\n",
        "# HYperparameter Grid\n",
        "param_dict = {'n_estimators' : n_estimators,\n",
        "              'max_depth' : max_depth,\n",
        "              'min_samples_split' : min_samples_split,\n",
        "              'min_samples_leaf' : min_samples_leaf}\n",
        "\n",
        "# Create an instance of the RandomForestClassifier\n",
        "rf_model1 = RandomForestClassifier()\n",
        "\n",
        "# Grid search\n",
        "rf_grid = GridSearchCV(estimator=rf_model1,\n",
        "                       param_grid = param_dict,\n",
        "                       cv = 5, verbose=2, scoring='f1')\n",
        "\n",
        "\n",
        "# Fit the Algorithm\n",
        "rf_grid.fit(x_train,y_train)\n",
        "\n",
        "\n",
        "\n",
        "# Predict on the model\n",
        "# Making predictions on train and test data\n",
        "train_class_preds = rf_grid.predict(x_train)\n",
        "test_class_preds = rf_grid.predict(x_test)"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Best: %f using %s\" % (rf_grid.best_score_, rf_grid.best_params_))"
      ],
      "metadata": {
        "id": "FFvFcrotrVTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Best: 0.698365 using {'max_depth': 8, 'min_samples_leaf': 50, 'min_samples_split': 100, 'n_estimators': 100}\n"
      ],
      "metadata": {
        "id": "FUGSek2bU8lQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart# Get the confusion matrix for both train and test\n",
        "\n",
        "labels = ['Responded', 'Not_Responded']\n",
        "cm = confusion_matrix(y_train, train_class_preds)\n",
        "print(cm)\n",
        "\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cm, annot=True, ax = ax) #annot=True to annotate cells\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels')\n",
        "ax.set_ylabel('True labels')\n",
        "ax.set_title('Confusion Matrix')\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)\n",
        ""
      ],
      "metadata": {
        "id": "K2YC4K9er0dj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the confusion matrix for both train and test\n",
        "\n",
        "labels = ['Responded', 'Not_Responded']\n",
        "cm = confusion_matrix(y_test, test_class_preds)\n",
        "print(cm)\n",
        "\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cm, annot=True, ax = ax) #annot=True to annotate cells\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels')\n",
        "ax.set_ylabel('True labels')\n",
        "ax.set_title('Confusion Matrix')\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)"
      ],
      "metadata": {
        "id": "HMTXfBK3r3AJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(metrics.classification_report(train_class_preds, y_train))\n",
        "print(\" \")\n",
        "\n",
        "print(\"roc_auc_score\")\n",
        "print(metrics.roc_auc_score(y_train, train_class_preds))\n",
        ""
      ],
      "metadata": {
        "id": "Ej057KBtsBcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.75      0.89      0.81    197475\n",
        "           1       0.81      0.62      0.70    153643\n",
        "\n",
        "    accuracy                           0.77    351118\n",
        "   macro avg       0.78      0.75      0.76    351118\n",
        "\n",
        "weighted avg       0.77      0.77      0.76    351118\n",
        "\n",
        "\n",
        "roc_auc_score\n",
        "0.7783553401864276\n"
      ],
      "metadata": {
        "id": "0kp2rzUMVJkM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hypertuned Random Forest\n",
        "print(metrics.classification_report(test_class_preds, y_test))\n",
        "print(\" \")\n",
        "\n",
        "print(\"roc_auc_score\")\n",
        "print(metrics.roc_auc_score(y_test, test_class_preds))\n",
        ""
      ],
      "metadata": {
        "id": "ctxfhKJ7sEXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.75      0.89      0.81     85077\n",
        "           1       0.81      0.62      0.70     65403\n",
        "\n",
        "    accuracy                           0.77    150480\n",
        "   macro avg       0.78      0.75      0.76    150480\n",
        "\n",
        "weighted avg       0.78      0.77      0.76    150480\n",
        "\n",
        "\n",
        "roc_auc_score\n",
        "0.7790602846747008"
      ],
      "metadata": {
        "id": "NeCdtLRhVlVg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearchCV which uses the Grid Search technique for finding the optimal hyperparameters to increase the model performance.\n",
        "\n",
        "our goal should be to find the best hyperparameters values to get the perfect prediction results from our model. But the question arises, how to find these best sets of hyperparameters? One can try the Manual Search method, by using the hit and trial process and can find the best hyperparameters which would take huge time to build a single model.\n",
        "\n",
        "For this reason, methods like Random Search, GridSearch were introduced. Grid Search uses a different combination of all the specified hyperparameters and their values and calculates the performance for each combination and selects the best value for the hyperparameters. This makes the processing time-consuming and expensive based on the number of hyperparameters involved.\n",
        "\n",
        "In GridSearchCV, along with Grid Search, cross-validation is also performed. Cross-Validation is used while training the model.\n",
        "\n",
        "That's why I have used GridsearCV method for hyperparameter optimization."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model seems to have decreased in performance since ROC_AUC score on test set was around 0.91 and after hyperparamter tuning\n",
        "the model seems to have performance with an accuracy of 0.77 on both the training and test sets. The precision and recall metrics are also quite good, with values above 0.75 for both classes on both the training and test sets. The F1-score, which is the harmonic mean of precision and recall, is also above 0.7 for both classes on both sets.\n",
        "\n",
        "The macro-average and weighted-average F1-scores are both around 0.76, indicating that the model is performing similarly well on both classes. The ROC AUC score of 0.779 on the test set is also decent, indicating that the model is able to discriminate between the positive and negative classes reasonably well."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "# Create an instance of the RandomForestClassifier\n",
        "xg_model = XGBClassifier()\n",
        "\n",
        "# Fit the Algorithm\n",
        "xg_models=xg_model.fit(x_train,y_train)\n",
        "\n",
        "# Predict on the model\n",
        "# Making predictions on train and test data\n",
        "\n",
        "train_class_preds = xg_models.predict(x_train)\n",
        "test_class_preds = xg_models.predict(x_test)\n",
        ""
      ],
      "metadata": {
        "id": "oy_Lh_PqFl-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Visualizing evaluation Metric Score chart# Get the confusion matrix for both train and test\n",
        "\n",
        "labels = ['Responded', 'Not_Responded']\n",
        "cm = confusion_matrix(y_train, train_class_preds)\n",
        "print(cm)\n",
        "\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cm, annot=True, ax = ax) #annot=True to annotate cells\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels')\n",
        "ax.set_ylabel('True labels')\n",
        "ax.set_title('Confusion Matrix')\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)\n",
        ""
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the confusion matrix for both train and test\n",
        "\n",
        "labels = ['Responded', 'Not_Responded']\n",
        "cm = confusion_matrix(y_test, test_class_preds)\n",
        "print(cm)\n",
        "\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cm, annot=True, ax = ax) #annot=True to annotate cells\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels')\n",
        "ax.set_ylabel('True labels')\n",
        "ax.set_title('Confusion Matrix')\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)"
      ],
      "metadata": {
        "id": "USihxsBussmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(metrics.classification_report(train_class_preds, y_train))\n",
        "print(\" \")\n",
        "\n",
        "print(\"roc_auc_score\")\n",
        "print(metrics.roc_auc_score(y_train, train_class_preds))\n",
        ""
      ],
      "metadata": {
        "id": "4NcFO_Axsuqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "            precision    recall  f1-score   support\n",
        "\n",
        "           0       0.78      0.89      0.83    204132\n",
        "           1       0.81      0.65      0.72    146986\n",
        "\n",
        "    accuracy                           0.79    351118\n",
        "   macro avg       0.79      0.77      0.78    351118\n",
        "\n",
        " weighted avg       0.79      0.79      0.78    351118\n",
        "\n",
        "\n",
        "roc_auc_score\n",
        "0.794894003685679"
      ],
      "metadata": {
        "id": "ITQOMIefmtwC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(metrics.classification_report(test_class_preds, y_test))\n",
        "print(\" \")\n",
        "\n",
        "print(\"roc_auc_score\")\n",
        "print(metrics.roc_auc_score(y_test, test_class_preds))\n",
        ""
      ],
      "metadata": {
        "id": "MQDi89m4swqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "            precision    recall  f1-score   support\n",
        "\n",
        "           0       0.77      0.88      0.82     87798\n",
        "           1       0.80      0.63      0.71     62682\n",
        "\n",
        "    accuracy                           0.78    150480\n",
        "   macro avg       0.78      0.76      0.77    150480\n",
        "weighted avg       0.78      0.78      0.77    150480\n",
        "\n",
        "\n",
        "roc_auc_score\n",
        "0.783971538025934"
      ],
      "metadata": {
        "id": "F-WyRc-dmzex"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at the results for the training set, the model achieved a precision of 0.78 for predicting the negative class (0) and 0.81 for predicting the positive class (1). Recall values were 0.89 and 0.65 for classes 0 and 1, respectively. The F1-score was 0.83 for class 0 and 0.72 for class 1. The weighted average F1-score was 0.78, which indicates that the model performed reasonably well overall.\n",
        "\n",
        "The model achieved an accuracy of 0.79 on the training set, which means that it correctly classified 79% of the instances. The ROC-AUC score was 0.7949, indicating that the model's ability to distinguish between positive and negative classes was good.\n",
        "\n",
        "On the test set, the model achieved a precision of 0.77 for class 0 and 0.80 for class 1. Recall values were 0.88 and 0.63 for classes 0 and 1, respectively. The F1-score was 0.82 for class 0 and 0.71 for class 1. The weighted average F1-score was 0.77, which is slightly lower than the training set.\n",
        "\n",
        "The model achieved an accuracy of 0.78 on the test set, which is slightly lower than the training set. The ROC-AUC score was 0.7839, which is slightly lower than the training set.\n",
        "\n",
        "Overall, the model seems to perform reasonably well, with comparable results on both the training and test sets. However, there is some room for improvement, especially in predicting the positive class, where the recall is lower than desired."
      ],
      "metadata": {
        "id": "0ecworlLGhDz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "# Number of trees\n",
        "n_estimators = [50,80,100]\n",
        "\n",
        "# Maximum depth of trees\n",
        "max_depth = [4,6,8]\n",
        "\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [50,100,150]\n",
        "\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [40,50]\n",
        "\n",
        "# HYperparameter Grid\n",
        "param_dict = {'n_estimators' : n_estimators,\n",
        "              'max_depth' : max_depth,\n",
        "              'min_samples_split' : min_samples_split,\n",
        "              'min_samples_leaf' : min_samples_leaf}\n",
        "\n",
        "# Create an instance of the RandomForestClassifier\n",
        "xg_model = XGBClassifier()\n",
        "\n",
        "# Fit the Algorithm\n",
        "# Grid search\n",
        "xg_grid = GridSearchCV(estimator=xg_model,\n",
        "                       param_grid = param_dict,\n",
        "                       cv = 5, verbose=2, scoring='roc_auc')\n",
        "\n",
        "xg_grid1=xg_grid.fit(x_train,y_train)\n",
        "# Predict on the model\n",
        "# Making predictions on train and test data\n",
        "\n",
        "train_class_preds = xg_grid1.predict(x_train)\n",
        "test_class_preds = xg_grid1.predict(x_test)"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Best: %f using %s\" % (xg_grid.best_score_, xg_grid.best_params_))\n"
      ],
      "metadata": {
        "id": "CxZm9yx_tLAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Visualizing evaluation Metric Score chart# Get the confusion matrix for both train and test\n",
        "\n",
        "labels = ['Retained', 'Churned']\n",
        "cm = confusion_matrix(y_train, train_class_preds)\n",
        "print(cm)\n",
        "\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cm, annot=True, ax = ax) #annot=True to annotate cells\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels')\n",
        "ax.set_ylabel('True labels')\n",
        "ax.set_title('Confusion Matrix')\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)\n",
        ""
      ],
      "metadata": {
        "id": "2HLDb6wktOx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the confusion matrix for both train and test\n",
        "\n",
        "labels = ['Retained', 'Churned']\n",
        "cm = confusion_matrix(y_test, test_class_preds)\n",
        "print(cm)\n",
        "\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cm, annot=True, ax = ax) #annot=True to annotate cells\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels')\n",
        "ax.set_ylabel('True labels')\n",
        "ax.set_title('Confusion Matrix')\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)"
      ],
      "metadata": {
        "id": "rLqpjMsgtRhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(metrics.classification_report(train_class_preds, y_train))\n",
        "print(\" \")\n",
        "\n",
        "print(\"roc_auc_score\")\n",
        "print(metrics.roc_auc_score(y_train, train_class_preds))\n",
        ""
      ],
      "metadata": {
        "id": "v5T1pTRZtTb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(metrics.classification_report(test_class_preds, y_test))\n",
        "print(\" \")\n",
        "\n",
        "print(\"roc_auc_score\")\n",
        "print(metrics.roc_auc_score(y_test, test_class_preds))\n",
        ""
      ],
      "metadata": {
        "id": "wYZKMQ47tVZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearchCV which uses the Grid Search technique for finding the optimal hyperparameters to increase the model performance.\n",
        "\n",
        "our goal should be to find the best hyperparameters values to get the perfect prediction results from our model. But the question arises, how to find these best sets of hyperparameters? One can try the Manual Search method, by using the hit and trial process and can find the best hyperparameters which would take huge time to build a single model.\n",
        "\n",
        "For this reason, methods like Random Search, GridSearch were introduced. Grid Search uses a different combination of all the specified hyperparameters and their values and calculates the performance for each combination and selects the best value for the hyperparameters. This makes the processing time-consuming and expensive based on the number of hyperparameters involved.\n",
        "\n",
        "In GridSearchCV, along with Grid Search, cross-validation is also performed. Cross-Validation is used while training the model.\n",
        "\n",
        "That's why I have used GridsearCV method for hyperparameter optimization."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DISCLAIMER : Couldn't Perform it as it was taking too much time to execute it**"
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I would like to go with both Recall and Precision and which describes both is F1 Score.\n",
        "\n",
        "To reduce false negative recall is important and to reduce false positives precision is important. Where both are important to be minimized, f1_score is being considered. False Positive is defined as the model predicted that the customer will churn but the customer didn't churn. But according to our model it will churn so, there would be quite chance of his churning not for immediate but after some times. So, for those type of customers we can send them some beneficial modified offers to retain them. Again false negative defines as model will predict that the customer won't churn but the customer really churned. That will be an issue for us. So, for that case we have to minimize the false negative. and false positive we must improve the score of both precision as well as recall which should direclt affect the f1_score positively. So, in our case recall will stand the higher but precision can't be neglected. so, *recall should be higher and f1_score should be moderate.*"
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RandomForestClassifier model is a good choice for this classification problem.\n",
        "\n",
        "Firstly, the model achieved a high accuracy of 0.99 on both the training and test sets, indicating that the model is correctly classifying a significant proportion of instances.\n",
        "\n",
        "Secondly, the precision and recall metrics are quite good, with values above 0.89 for both classes on both the training and test sets. This means that the model is able to correctly identify true positives while minimizing false positives and false negatives.\n",
        "\n",
        "Thirdly, 88he F1-score, which is a combined metric of precision and recall, is above 0.7 for both classes on both sets. This indicates that the model has a good balance between precision and recall for both classes, which is important in a classification problem where both classes are equally important.\n",
        "\n",
        "Furthermore, the macro-average and weighted-average F1-scores are both around 0.87, which means that the model is performing similarly well on both classes. This is important because it indicates that the model is not biased towards either class and is able to classify both classes equally well.\n",
        "\n",
        "Lastly, the ROC AUC score of 0.91 on the test set is also a good metric to evaluate the performance of a classification model. This indicates that the model has a good ability to distinguish between positive and negative classes, which is important in this binary classification problem.\n",
        "\n",
        "In summary, the RandomForestClassifier model is a good choice for this classification problem due to its high accuracy, good precision and recall metrics, high F1-scores, unbiased performance on both classes, and good ROC AUC score."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest algorithm reports feature importance by considering feature usage over all the trees in the forest.This gives good insigt into which features have important information with respect to the outcome variable.It uses \"Gini impurity reduction\" or \"mean decrease impurity\" for calculating the importance.\n",
        "\n",
        "Feature importance is calculated for a feature by multiplying error reduction at any node by the feature with the proportion of samples reaching that node. Then the values are averaged over all the trees to find feature importance.\n",
        "\n",
        "In sklearn , the classifier returns a parameter called featureimportances,which\n",
        "holds the feature importance values."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create importance feature variable\n",
        "importances = rf_model.feature_importances_\n",
        "\n",
        "#Create a Dataframe to store the features and their coreesponding importances\n",
        "importance_dict = {'Feature' : list(x_train.columns),\n",
        "                   'Feature_Importance' : importances}\n",
        "importance_df = pd.DataFrame(importance_dict)\n",
        "\n",
        "#Rounding the Feature Importance\n",
        "importance_df['Feature_Importance'] = round(importance_df['Feature_Importance'],2)\n",
        ""
      ],
      "metadata": {
        "id": "3AeP52MhuGsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Sorting the features based on their importances with most important feature at the top.\n",
        "importance_df = importance_df.sort_values(by=['Feature_Importance'],ascending=False)\n",
        ""
      ],
      "metadata": {
        "id": "3ZKJ83EcunH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (8,6))\n",
        "#plot the values\n",
        "sns.barplot(y='Feature',x='Feature_Importance', data = importance_df)"
      ],
      "metadata": {
        "id": "kW64h1Fbvmqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The top five features being Vintage_Treated,Annual_Premium_Treated,Vehicle_Damage,Age,Previously_Insured.The importance is normalised and shows the relative importance of features."
      ],
      "metadata": {
        "id": "mWoKjJNsyVTE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LIME**"
      ],
      "metadata": {
        "id": "axjdBZaWvOcT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LIME (Local Interpretable Model-agnostic Explanations) is a popular method for explaining the predictions of machine learning models. It provides a way to explain the relationship between input features and model predictions at a local level, which can be especially useful for understanding the behavior of complex models like Random Forests.\n",
        "\n",
        "One of the main advantages of LIME is its model-agnostic approach, meaning that it can be applied to any machine learning model regardless of its underlying algorithm or complexity. LIME works by approximating the model's behavior in the local vicinity of a particular instance, generating a simpler \"local\" model that can be more easily interpreted by humans. This allows users to identify which input features are most important for a particular prediction, and how they are influencing the output.\n",
        "\n",
        "Another advantage of LIME is that it provides a flexible framework for visualizing and interpreting feature importance scores. The LimeTabularExplainer object used in the code above, for example, allows users to generate feature importance scores and visualize them as a bar chart, making it easy to identify which features are having the greatest impact on the model's predictions.\n",
        "\n",
        "Overall, LIME is a powerful and flexible tool for interpreting the behavior of machine learning models, making it a popular choice for data scientists and machine learning practitioners.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pf58iYvRvSMo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lime"
      ],
      "metadata": {
        "id": "WP9Ckfadvb-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import lime\n",
        "import lime.lime_tabular\n",
        "\n"
      ],
      "metadata": {
        "id": "1KNvjh8GvS0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the LimeTabularExplainer object\n",
        "explainer = lime.lime_tabular.LimeTabularExplainer(x_train.values, feature_names=list(x_train.columns), class_names=['0', '1'])\n",
        "# Select a random instance from the test data\n",
        "instance = x_test.iloc[0]\n",
        "\n",
        "# Generate feature importance scores using Lime\n",
        "exp = explainer.explain_instance(instance.values, rf_model.predict_proba, num_features=len(x_train.columns))\n"
      ],
      "metadata": {
        "id": "--WzCVwpvUzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the feature names and scores from the Lime explanation\n",
        "features, scores = zip(*exp.as_list())\n",
        "\n",
        "# Create a horizontal bar chart of the feature importances\n",
        "fig, ax = plt.subplots(figsize=(10,6))\n",
        "ax.barh(features, scores, color='blue')\n",
        "\n",
        "# Add labels and title\n",
        "ax.set_xlabel('Importance Score')\n",
        "ax.set_ylabel('Feature')\n",
        "ax.set_title('Feature Importance')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-57TjH2ZwHeZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Key Insights:**\n",
        "\n",
        "The highest count of customers falls within the age range of 20-30 for both males and females.\n",
        "\n",
        " Customers over 40 years of age are more likely to respond to insurance buying.\n",
        "Mean age for Response 1 (interested in buying insurance) is around 42, and for\n",
        "\n",
        "Response 0 (not interested), it is around 35.\n",
        "\n",
        "Region code 28.0 has the maximum customers, followed by Region_C and Region_E.\n",
        "The middle-aged adult age group has the highest number of responses, followed by the young adult and senior groups.\n",
        "\n",
        "Policy Sales Channel D has the highest count compared to other channels.\n",
        "\n",
        "Male customers have a higher count than female customers in both Response categories, suggesting that male customers may be more responsive to the company's outreach efforts.\n",
        "\n",
        "Vehicles older than two years appear to be subject to higher premiums than others, and customers with damaged vehicles are willing to pay a higher premium for coverage.\n",
        "\n",
        "Almost all customers (99.79%) had a driving license, and 45.82% of customers had previously purchased insurance.\n",
        "\n",
        "**Business Solutions:**\n",
        "\n",
        "Target marketing efforts towards the age group of 20-30 for both males and females as they form the highest count of customers.\n",
        "\n",
        "Create customized insurance plans for customers over 40 years of age to target their specific needs and interests.\n",
        "\n",
        "Devote more resources to Region code 28.0, Region_C and Region_E as they have the maximum customers.\n",
        "\n",
        "Focus on marketing efforts towards the middle-aged adult age group as they have the highest number of responses.\n",
        "\n",
        "Devote resources to Policy Sales Channel D as it has the highest count compared to other channels.\n",
        "\n",
        "Offer incentives for customers who have previously purchased insurance to encourage them to continue with the company.\n",
        "\n",
        "Consider partnering with driving schools to target potential new customers and build brand awareness among them.\n"
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}