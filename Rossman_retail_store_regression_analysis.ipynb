{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Krishanu-Saha/data-science/blob/main/Rossman_retail_store_regression_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - ROSSMAN RETAIL STORE REGRESSION ANALYSIS\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Regression\n",
        "##### **Contribution**    - BY KRISHANU SAHA, Individual\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Rossman retail store regression analysis is a machine learning project that aims to predict the daily sales of different retail stores using various features such as promotions, holidays, and store information. The project uses several machine learning algorithms, including Ridge regression, Lasso regression, Random Forest, and Gradient Boosting Regressor, to identify the best model for the prediction task. The final model is then used to generate sales predictions for a test dataset, which are compared to the actual sales figures to assess the model's performance.\n",
        "\n",
        "The dataset used in this project contains information about 1,115 Rossmann stores across different countries, including their daily sales figures, promotional activities, holidays, and store information. The dataset has a total of 1,017,209 observations, with 18 features that describe each store's daily sales.\n",
        "\n",
        "The first step of the analysis is to perform exploratory data analysis (EDA) to gain an understanding of the data and identify any patterns or trends. The EDA involves examining the distribution of the target variable (i.e., daily sales) and exploring the relationships between the target variable and the different features. It also involves visualizing the data using various plots and charts to identify any outliers, missing values, or other anomalies.\n",
        "\n",
        "After completing the EDA, the project moves on to feature selection and engineering. This involves identifying the most important features that have the greatest impact on daily sales and creating new features by combining or transforming the existing ones. The feature engineering process also involves data cleaning and preprocessing, such as handling missing values and converting categorical features into numerical ones using one-hot encoding.\n",
        "\n",
        "Once the data is cleaned and preprocessed, the next step is to train and evaluate several machine learning models using different algorithms. The models are evaluated using several metrics, including mean squared error (MSE), mean absolute error (MAE), and R-squared (R2) score. Ridge regression and Lasso regression are used as baseline models, and Random Forest and Gradient Boosting Regressor are used as more complex models to compare their performance.\n",
        "\n",
        "After comparing the performance of different models, the Gradient Boosting Regressor is found to be the best model for the prediction task, with an R2 score of 0.93 on the test dataset. The model is then used to generate sales predictions for the test dataset and compared to the actual sales figures.\n",
        "\n",
        "Finally, the project uses the SHAP (SHapley Additive exPlanations) explainability tool to interpret the Gradient Boosting Regressor model and understand the feature importance. SHAP values provide an estimate of how much each feature contributes to the model's output, and they help to identify the most important features that have the greatest impact on daily sales. The SHAP summary plot shows the most important features ranked by their impact on the model's output, providing insights into the key drivers of daily sales.\n",
        "\n",
        "In conclusion, the Rossman retail store regression analysis is a machine learning project that demonstrates the application of various algorithms, including Ridge regression, Lasso regression, Random Forest, and Gradient Boosting Regressor, to predict the daily sales of retail stores. The project uses a comprehensive approach that involves exploratory data analysis, feature selection and engineering, model training and evaluation, and explainability analysis using SHAP. The project results in a robust machine learning model that provides accurate sales predictions and identifies the most important features that impact daily sales"
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[PROJECT GIT HUB LINK IS PRESENT IN THIS TEXT, CLICK HERE](https://github.com/Krishanu-Saha/data-science/blob/main/Rossman_retail_store_regression_analysis.ipynb)"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Rossmann operates over 3,000 drug stores in 7 European countries. Currently, Rossmann store managers are tasked with predicting their daily sales for up to six weeks in advance. Store sales are influenced by many factors, including promotions, competition, school and state holidays, seasonality, and locality. With thousands of individual managers predicting sales based on their unique circumstances, the accuracy of results can be quite varied. We are provided with historical sales data for 1,115 Rossmann stores. The task is to forecast the \"Sales\" column for the test set.**"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "X-zAtxegW4JM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from datetime import datetime\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from statsmodels.graphics.regressionplots import influence_plot\n",
        "import statsmodels.formula.api as smf\n",
        "from statsmodels.graphics.regressionplots import plot_regress_exog\n",
        "from statsmodels.graphics.regressionplots import plot_leverage_resid2\n",
        "from scipy.stats import zscore \n",
        "from sklearn.preprocessing import StandardScaler \n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.linear_model import Ridge, Lasso\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "sales_df = pd.read_csv('/content/drive/MyDrive/Almabetter /project/REGRESSION/Rossmann Stores Data.csv')\n",
        "stores_df = pd.read_csv('/content/drive/MyDrive/Almabetter /project/REGRESSION/store.csv')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "sales_df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stores_df.head()"
      ],
      "metadata": {
        "id": "BsR34Z_dwhSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "sales_df.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stores_df.shape"
      ],
      "metadata": {
        "id": "pnMIc5oQw5ST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The sales dataset contains 1017209 rows and 9 columns whereas stores dataset contains 1115 rows and 10 columns."
      ],
      "metadata": {
        "id": "JjlNiGYLOqF_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sales_df.info()"
      ],
      "metadata": {
        "id": "zMRs9MNsPkym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stores_df.info()"
      ],
      "metadata": {
        "id": "nPsV5aTcxIgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Number of duplicated data\n",
        "len(stores_df[stores_df.duplicated()])"
      ],
      "metadata": {
        "id": "B5ptV7avx36R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(sales_df[sales_df.duplicated()])"
      ],
      "metadata": {
        "id": "BjNSuyN5P4TM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have zero duplicate rows.Well thats a good sign!"
      ],
      "metadata": {
        "id": "4Gf3q1HjZMzb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking null values for every column\n",
        "stores_df.isnull().sum()"
      ],
      "metadata": {
        "id": "wRGh8BLmyA1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales_df.isnull().sum()"
      ],
      "metadata": {
        "id": "_HpXoKCmQHJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Handling Missing and Null Values\n"
      ],
      "metadata": {
        "id": "ETaklk-BH4fW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# filling competition distance with the median value\n",
        "stores_df['CompetitionDistance'].fillna(stores_df['CompetitionDistance'].median(), inplace = True)\n",
        "     "
      ],
      "metadata": {
        "id": "W4T3fmLt0MHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filling competition open since month and year with the most occuring values of the columns i.e modes of those columns\n",
        "stores_df['CompetitionOpenSinceMonth'].fillna(stores_df['CompetitionOpenSinceMonth'].mode()[0], inplace = True)\n",
        "stores_df['CompetitionOpenSinceYear'].fillna(stores_df['CompetitionOpenSinceYear'].mode()[0], inplace = True)"
      ],
      "metadata": {
        "id": "2kuYB4y90URY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imputing the nan values of promo2 related columns with 0\n",
        "stores_df['Promo2SinceWeek'].fillna(value=0,inplace=True)\n",
        "stores_df['Promo2SinceYear'].fillna(value=0,inplace=True)\n",
        "stores_df['PromoInterval'].fillna(value=0,inplace=True)\n",
        "     "
      ],
      "metadata": {
        "id": "Sb-jDPwA0tps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#merge the datasets on stores data\n",
        "df = sales_df.merge(right=stores_df, on=\"Store\", how=\"left\")\n",
        "     "
      ],
      "metadata": {
        "id": "SYrt8rKS09R4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#first five rows of the merged dataset\n",
        "df.head()\n",
        "     "
      ],
      "metadata": {
        "id": "hRZshLRJ1AsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#shape of the dataframe\n",
        "df.shape"
      ],
      "metadata": {
        "id": "3aNfD9td1Rgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#datatypes\n",
        "df.info()"
      ],
      "metadata": {
        "id": "XSFGE4OB1S0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to change certain column datatypes .date ,Stateholiday"
      ],
      "metadata": {
        "id": "O_6i71fi2syB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['StateHoliday'].unique()"
      ],
      "metadata": {
        "id": "KxYD1XAiRQxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have to convert values to zero or one appropriately"
      ],
      "metadata": {
        "id": "LdRZTQ9nRa5a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature engineering"
      ],
      "metadata": {
        "id": "CpcPU7g5IP2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#change into int type\n",
        "df['StateHoliday'] = df['StateHoliday'].replace({'0':0,'a':1,'b':1,'c':1})"
      ],
      "metadata": {
        "id": "sIJLu2c15415"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Converting Date column into datetime datatype.\n",
        "df['Date'] = pd.to_datetime(df['Date'])"
      ],
      "metadata": {
        "id": "DUo35vm33dkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#creating features from the date\n",
        "df['Year'] = df['Date'].dt.year\n",
        "df['Month'] = df['Date'].dt.month\n",
        "df['WeekOfYear'] = df['Date'].dt.weekofyear\n",
        "df['DayOfYear'] = df['Date'].dt.dayofyear\n",
        "years = df['Year'].unique()\n",
        "years"
      ],
      "metadata": {
        "id": "S81EWmbK6yni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have obtained a dataset consisting of 1017209 rows and 22 columns. The target variable of our analysis is the 'Sales' column. The dataset is free of any duplicate entries or missing values."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "columns = list(df.columns)\n",
        "columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "dGcIQFNrHc_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description "
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Store: An integer value representing the unique identifier for each store in the dataset.\n",
        "\n",
        "DayOfWeek: An integer value representing the day of the week (1-7) when the sale was recorded.\n",
        "\n",
        "Date: A date value representing the date when the sale was recorded.\n",
        "\n",
        "Sales: A numerical value representing the amount of sales in a given day for a particular store.\n",
        "\n",
        "Customers: An integer value representing the number of customers who made purchases on a particular day at a particular store.\n",
        "\n",
        "Open: A binary value (0 or 1) indicating whether a store was open or closed on a given day.\n",
        "\n",
        "Promo: A binary value (0 or 1) indicating whether a store was running a promotional offer on a given day.\n",
        "\n",
        "StateHoliday: A categorical variable indicating the type of state holiday (if any) on a given day.\n",
        "\n",
        "SchoolHoliday: A binary value (0 or 1) indicating whether a school holiday was on a given day.\n",
        "\n",
        "StoreType: A categorical variable indicating the type of store.\n",
        "\n",
        "Assortment: A categorical variable indicating the level of assortment (i.e., range of products) offered by a store.\n",
        "\n",
        "CompetitionDistance: A numerical value representing the distance (in meters) to the nearest competitor store.\n",
        "\n",
        "CompetitionOpenSinceMonth: An integer value representing the month when the nearest competitor store opened.\n",
        "\n",
        "CompetitionOpenSinceYear: An integer value representing the year when the nearest competitor store opened.\n",
        "\n",
        "Promo2: A binary value (0 or 1) indicating whether a store is participating in a continuous promotional offer (i.e., Promo2).\n",
        "\n",
        "Promo2SinceWeek: An integer value representing the week when the store started participating in the continuous promotional offer (i.e., Promo2).\n",
        "\n",
        "Promo2SinceYear: An integer value representing the year when the store started participating in the continuous promotional offer (i.e., Promo2).\n",
        "\n",
        "PromoInterval: A categorical variable indicating the interval of continuous promotional offers, if any.\n",
        "\n",
        "Year: An integer value representing the year of the recorded sale.\n",
        "\n",
        "Month: An integer value representing the month of the recorded sale.\n",
        "\n",
        "WeekOfYear: An integer value representing the week of the year when the sale was recorded.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "df.nunique()"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "from the observtion it seems that StateHoliday,StoreType,Assortment and PromoInterval are catagorical columns but we have to further investigate tthe StateHoiday column."
      ],
      "metadata": {
        "id": "ly79GVcmQeC_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***EDA***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### UNIVARIATE ANALYSIS"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop all rows where \"Open\" is equal to zero\n",
        "data = df[df['Open'] != 0]"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the counts of each store type\n",
        "store_counts = stores_df['StoreType'].value_counts()\n",
        "\n",
        "# Create a pie chart using the store type counts\n",
        "plt.figure(figsize = (12,8))\n",
        "plt.pie(store_counts, labels=store_counts.index, autopct='%1.1f%%')\n",
        "\n",
        "# Set the title of the plot\n",
        "plt.title('Store Types')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gL_IKRNLxGs0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reason for choosing pie plot:**It represents data visually as a fractional part of a whole, which can be an effective communication tool for the even uninformed audience. It enables the audience to see a data comparison at a glance to make an immediate analysis or to understand information quickly."
      ],
      "metadata": {
        "id": "MNtnZsoxVMjC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**INSIGHTS :**"
      ],
      "metadata": {
        "id": "6MRYC-yFa2RN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "store A type : 54%\n",
        " \n",
        " store B type : 1.5%\n",
        " \n",
        " store C type : 13.3%\n",
        " \n",
        " store D type : 31.2%"
      ],
      "metadata": {
        "id": "yU8jIlSeWh_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the counts of each store type\n",
        "store_counts = stores_df['Assortment'].value_counts()\n",
        "\n",
        "# Create a pie chart using the store type counts\n",
        "plt.figure(figsize = (12,8))\n",
        "plt.pie(store_counts, labels=store_counts.index, autopct='%1.1f%%')\n",
        "\n",
        "# Set the title of the plot\n",
        "plt.title('Assortment Types')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uh5Y4rvANjYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reason for choosing pie plot:**It represents data visually as a fractional part of a whole, which can be an effective communication tool for the even uninformed audience. It enables the audience to see a data comparison at a glance to make an immediate analysis or to understand information quickly."
      ],
      "metadata": {
        "id": "XXiYjBNfVk3j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**INSIGHTS :**"
      ],
      "metadata": {
        "id": "R9IQjHSZbPma"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assortment type a : 53.2%\n",
        "Assortment type b : 0.8%\n",
        "Assortment type c : 46.0%\n"
      ],
      "metadata": {
        "id": "ueW9wgq_XeBW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CUSTOMER ANALYSIS"
      ],
      "metadata": {
        "id": "6SCQ8vPjgHKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "UH8ZxwbgZLzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Set the title and axis labels\n",
        "plt.figure(figsize = (20,20))\n",
        "\n",
        "# Create the barplot using seaborn\n",
        "plt.scatter(x = df['Customers'],y = df['Sales'])\n",
        "plt.title('Total Customers by Store')\n",
        "plt.xlabel('customers')\n",
        "plt.ylabel('sales')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "g5FXwS2YZVX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reason of chosing scatter plot:** A scatter plot is often used to visualize the relationship between two continuous variables, such as the average daily rate (ADR) and total stay. It shows the pattern of how the variables are related and can help to identify any correlations or outliers. Additionally, scatter plots can help to reveal any underlying trends or structures in the data, making it easier to understand the relationships between the variables."
      ],
      "metadata": {
        "id": "pTjekT-ZVwCD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**INSIGHTS :**"
      ],
      "metadata": {
        "id": "ZDeklV2rbR3S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can observe there is a direct relationship between customers and Sales , which is obvious more customers means more sales."
      ],
      "metadata": {
        "id": "QcqOAmUIhfPJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a bar plot with the Promo categories on the x-axis and the total number of customers on the y-axis\n",
        "sns.barplot(x = 'Promo', y = 'Customers',data = df)\n",
        "plt.title('Mean Customers by Promo')\n",
        "plt.xlabel('Promo')\n",
        "plt.ylabel('Customers')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "axQuJcfAfMBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reason of chosing bar plot :**A bar plot is a good choice to visualize the number of bookings versus the agent because it allows for a quick and easy comparison of the frequency or count of the number of bookings made by each agent. It is particularly useful when you want to compare categories or groups, such as different agents, and see how they stack up against each other. The bar plot also provides a clear visual representation of the distribution of the data and makes it easy to identify any trends or patterns in the number of bookings made by each agent."
      ],
      "metadata": {
        "id": "1qnEj0VsWE7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**INSIGHTS :**"
      ],
      "metadata": {
        "id": "fe_0zMFZbSw-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Store owners who are promoting their stores has more customers than those who do not."
      ],
      "metadata": {
        "id": "Iwv9RvRejXag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a bar plot with the StateHoliday categories on the x-axis and the total number of customers on the y-axis\n",
        "sns.barplot(x = 'StateHoliday', y = 'Customers',data = df)\n",
        "plt.title('Mean Customers by StateHoliday')\n",
        "plt.xlabel('StateHoliday')\n",
        "plt.ylabel('Customers')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MWB2OKbOgVxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reason of chosing bar plot :**A bar plot is a good choice to visualize the number of bookings versus the agent because it allows for a quick and easy comparison of the frequency or count of the number of bookings made by each agent. It is particularly useful when you want to compare categories or groups, such as different agents, and see how they stack up against each other. The bar plot also provides a clear visual representation of the distribution of the data and makes it easy to identify any trends or patterns in the number of bookings made by each agent."
      ],
      "metadata": {
        "id": "TmWgD_ttWoOX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**INSIGHTS :**"
      ],
      "metadata": {
        "id": "jtBRuKFnbTjS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Customers seems to be purchasing more likely at a Stateholiday"
      ],
      "metadata": {
        "id": "YZcpNxkXkOca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a bar plot with the SchoolHoliday categories on the x-axis and the total number of customers on the y-axis\n",
        "sns.barplot(x = 'SchoolHoliday', y = 'Customers',data = df)\n",
        "plt.title('Mean Customers by SchoolHoliday')\n",
        "plt.xlabel('SchoolHoliday')\n",
        "plt.ylabel('Customers')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "g0vCZwxMgWGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reason of chosing bar plot :**A bar plot is a good choice to visualize the number of bookings versus the agent because it allows for a quick and easy comparison of the frequency or count of the number of bookings made by each agent. It is particularly useful when you want to compare categories or groups, such as different agents, and see how they stack up against each other. The bar plot also provides a clear visual representation of the distribution of the data and makes it easy to identify any trends or patterns in the number of bookings made by each agent."
      ],
      "metadata": {
        "id": "gwCL_-nXWqXP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**INSIGHTS :**"
      ],
      "metadata": {
        "id": "kwV-mYQFbUdI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Customers are equally likely to come out on when it is a Schoolday."
      ],
      "metadata": {
        "id": "qgk0Hl7gk075"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a bar plot with the StoreType categories on the x-axis and the total number of customers on the y-axis\n",
        "sns.barplot(x = 'StoreType', y = 'Customers',data = df)\n",
        "plt.title('Mean Customers by StoreType')\n",
        "plt.xlabel('StoreType')\n",
        "plt.ylabel('Customers')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aHQtdao3gWaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reason of chosing bar plot**:A bar plot is a good choice to visualize the number of bookings versus the agent because it allows for a quick and easy comparison of the frequency or count of the number of bookings made by each agent. It is particularly useful when you want to compare categories or groups, such as different agents, and see how they stack up against each other. The bar plot also provides a clear visual representation of the distribution of the data and makes it easy to identify any trends or patterns in the number of bookings made by each agent."
      ],
      "metadata": {
        "id": "mj9kjtmKWsVJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**INSIGHTS :**"
      ],
      "metadata": {
        "id": "z4S7_IEwbVQb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mean customers per StoreType in StoreType B is highest.It may indicate it is in demand or as from the pie chart B Type store are lesser in number thats why the high demand. "
      ],
      "metadata": {
        "id": "jF7c4IkEng0N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a bar plot with the StoreType categories on the x-axis and the total number of customers on the y-axis\n",
        "sns.barplot(x = 'Assortment', y = 'Customers',data = df)\n",
        "plt.title('mean Customers by Assortment')\n",
        "plt.xlabel('Assortment')\n",
        "plt.ylabel('Customers')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Sb50Kd34mSLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reason of chosing bar plot** :A bar plot is a good choice to visualize the number of bookings versus the agent because it allows for a quick and easy comparison of the frequency or count of the number of bookings made by each agent. It is particularly useful when you want to compare categories or groups, such as different agents, and see how they stack up against each other. The bar plot also provides a clear visual representation of the distribution of the data and makes it easy to identify any trends or patterns in the number of bookings made by each agent."
      ],
      "metadata": {
        "id": "ip-3LKxJWuda"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**INSIGHTS :**"
      ],
      "metadata": {
        "id": "7jV7KECPbZSY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stores which are of assortment type b are in huge demand since Mean Customers are highest in that category."
      ],
      "metadata": {
        "id": "AzR42LY-peQq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the list of columns to plot\n",
        "col_count = ['DayOfWeek',  'Promo', 'StateHoliday', 'SchoolHoliday',\n",
        "             'StoreType', 'Assortment', 'CompetitionOpenSinceMonth',\n",
        "             'CompetitionOpenSinceYear', 'Promo2', 'Promo2SinceWeek',\n",
        "             'Promo2SinceYear', 'PromoInterval', 'Year', 'Month',\n",
        "             'WeekOfYear']\n",
        "\n",
        "# Create bar plots of the mean sales for each value in each column\n",
        "for col in col_count:\n",
        "    plt.figure(figsize=(12,6))\n",
        "    data.groupby(col)['Sales'].mean().plot(kind='bar')\n",
        "    plt.title('Mean Sales vs. ' + col)\n",
        "    plt.xlabel(col)\n",
        "    plt.ylabel('Mean Sales')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "mS_fiOhShRg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reason of chosing bar plot :**A bar plot is a good choice to visualize the number of bookings versus the agent because it allows for a quick and easy comparison of the frequency or count of the number of bookings made by each agent. It is particularly useful when you want to compare categories or groups, such as different agents, and see how they stack up against each other. The bar plot also provides a clear visual representation of the distribution of the data and makes it easy to identify any trends or patterns in the number of bookings made by each agent."
      ],
      "metadata": {
        "id": "KByaJA7ysEXx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**INSIGHTS :**"
      ],
      "metadata": {
        "id": "CCG9p9-8bhhW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Graph 1 : Mean Sales vs Dayofweek** : Mean Sales are highest in day 1 and day 7 , and lowest in day 6 . This may indicate that people wait for the Hoilday to come and go for shopping. "
      ],
      "metadata": {
        "id": "mx7yUMFEq2uS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Graph 2 : Mean Sales vs Promo** Means sales are higher when store owner promote their shops."
      ],
      "metadata": {
        "id": "f84H7M3gr0nW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Graph 3 : Mean Sales vs StateHoliday** Mean Sales are higher when there is a sateholiday."
      ],
      "metadata": {
        "id": "R5g_xbsNsDTj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Graph 4 : Mean Sales vs StateHoliday** There isn't much difference in mean Sales regards to StateHoliday."
      ],
      "metadata": {
        "id": "ZAjlbrYtsJk6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Graph 5 : Mean Sales vs StoreType** Means Sales are highest in StoreType b ,it can result into greater yield in profit. "
      ],
      "metadata": {
        "id": "P44VyhCssKua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Graph 6 : Mean Sales vs Assortment** Means Sales are higher in stores which have assortments of Type b. "
      ],
      "metadata": {
        "id": "ncZSizW4sNMa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Graph 7 : Mean Sales vs CompetitionOpenSinceMonth** There isn't much variation in Mean Sales with CompetitionOpenSinceMonth. "
      ],
      "metadata": {
        "id": "X7z6DtzisMF6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Graph 8 : Mean Sales vs CompetitionOpenSinceYear** We can observe that there is gradual decrease in mean sales as new competition has opened in recent years."
      ],
      "metadata": {
        "id": "MP01gG_osOkl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Graph 9 : Mean Sales vs Promo2** There seems to be a negetive effect on Sales where stores continued to do promotions."
      ],
      "metadata": {
        "id": "reavU60ksQIT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Graph 10 : Mean Sales vs Promo2sinceweek** There seems to be no pattern between Promo2sinceweek and mean sales."
      ],
      "metadata": {
        "id": "W-Q4eyl-sRZK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Graph 11 : Mean Sales vs Promo2sinceweek** We can observe a gradual decrease in sales from 2009 to uptil 2013,then there is a slight bump at 2014 then again decrease in sales at 2015."
      ],
      "metadata": {
        "id": "7UmXTOoc6ZD2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Graph 12 : Mean Sales vs Promointerval**  When promotions are run from the start of the year (january)...we can obtain higher sales in (jan-oct) interval"
      ],
      "metadata": {
        "id": "Fof0BCnV7CcW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Graph 13 : Mean Sales vs year** Means sales have increased over the years."
      ],
      "metadata": {
        "id": "wnnqOz8O7zrQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Graph 14 : Mean Sales vs Promo2sinceweek** Mean sales are higher towards the end of the year.mainly in Oct, Nov and Dec. "
      ],
      "metadata": {
        "id": "rEBh3d5L8GiJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's check the relationship between store type, assortment levels and sales\n",
        "sns.barplot(x=data[\"StoreType\"],y=data['Sales'],hue=df[\"Assortment\"])"
      ],
      "metadata": {
        "id": "VlwebWFSy27l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reason of chosing bar plot :**A bar plot is a good choice to visualize the number of bookings versus the agent because it allows for a quick and easy comparison of the frequency or count of the number of bookings made by each agent. It is particularly useful when you want to compare categories or groups, such as different agents, and see how they stack up against each other. The bar plot also provides a clear visual representation of the distribution of the data and makes it easy to identify any trends or patterns in the number of bookings made by each agent."
      ],
      "metadata": {
        "id": "bqhPtsOiW0mP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "from the bar graph above We can observe that Assortment type b is only availaible in store type b.\n",
        "\n",
        "In storetype b , assortment c yields greater mean sales and can be used to extract profit if it is in large quantities."
      ],
      "metadata": {
        "id": "ZudCg9I-9Pkg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.factorplot(data = df, x =\"Month\", y = \"Sales\",\n",
        "               col = 'StoreType' ,\n",
        "               hue ='Promo',\n",
        "               row = \"Year\"\n",
        "             )"
      ],
      "metadata": {
        "id": "72fI2WKi0PNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Factor Plot is chosen** to draw a different types of categorical plot in a single frame. we can visualise complex data in much more simple way which has multiple layers of data.  "
      ],
      "metadata": {
        "id": "zHanh9RgXm8s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**INSIGHTS :**"
      ],
      "metadata": {
        "id": "SSjvSOMDbkE-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Every type of store has the same kind of trend throughout the year. Sales are generally increased towards the end of the year."
      ],
      "metadata": {
        "id": "vBj2T4zb_It6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the Year and Month columns into a single date column\n",
        "data['Date'] = pd.to_datetime(data['Year'].astype(str) + '-' + data['Month'].astype(str) + '-1')\n",
        "\n",
        "# Group the data by year and month and compute the total sales for each group\n",
        "sales_by_year_month = df.groupby(['Year', 'Month'])['Sales'].sum()\n",
        "\n",
        "# Create a figure and axis for the plot\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "\n",
        "# Loop through each year and plot the monthly sales as a line on the same axis\n",
        "for year in sales_by_year_month.index.levels[0]:\n",
        "    sales_by_month = sales_by_year_month.loc[year]\n",
        "    ax.plot(sales_by_month.index, sales_by_month.values, label=str(year))\n",
        "\n",
        "# Add axis labels and a legend to the plot\n",
        "ax.set_xlabel('Month')\n",
        "ax.set_ylabel('Total Sales')\n",
        "ax.set_title('Monthly Sales Over Time by Year')\n",
        "ax.legend()\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6fDzuRv-YGaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reason for chosing line plot:** to analyse the trend of Sales throughout the year which is very effective and easy process  through line graphs."
      ],
      "metadata": {
        "id": "ylqGkJMgYDtW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**INSIGHTS :**"
      ],
      "metadata": {
        "id": "bAIc2diublfT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The time frame between 10-12 month period is where we can expect increase in sales. "
      ],
      "metadata": {
        "id": "nobhJ1lG_2Xw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.columns\n"
      ],
      "metadata": {
        "id": "a-pmtsENOUsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_columns = [ 'DayOfWeek', 'Sales', 'Customers',  'CompetitionDistance', 'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear', 'Year', 'Month', 'WeekOfYear',  'DayOfYear']\n",
        "        \n",
        " # Create a correlation matrix of the numerical columns\n",
        "correlation_matrix = data[num_columns].corr()\n",
        "\n",
        "# Create the heatmap\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.heatmap(correlation_matrix, annot=True)\n",
        "\n",
        "# Set the title of the plot\n",
        "plt.title('Correlation Heatmap')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()      \n",
        "     "
      ],
      "metadata": {
        "id": "xqlVkgMqOiHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reason for chosing heatmap**: to understand the correlation between viriables in the dataframe visually where bright colour represents high correlation and dark correlation represents less or negetive correlation."
      ],
      "metadata": {
        "id": "abotM45pY3Fn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**INSIGHTS :**"
      ],
      "metadata": {
        "id": "b9KJvpMdbmy9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is a high corelation between customers and Sales which is obvious more customer means more sales."
      ],
      "metadata": {
        "id": "HXSmjSW4BVvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HYPOTHESIS TESTING "
      ],
      "metadata": {
        "id": "9y_BUE5TSMix"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1) The effect of promotions on sales: We are testing whether there is a significant difference in sales between days when there is a promotion versus days when there is no promotion.**\n",
        "\n",
        "Null hypothesis: There is no significant difference in sales between stores with and without a promotion.\n",
        "\n",
        "Alternative hypothesis: Stores with a promotion have significantly higher sales than stores without a promotion."
      ],
      "metadata": {
        "id": "4BXOMIObSWKn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = data.copy()"
      ],
      "metadata": {
        "id": "pFjpOWFQVqf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.stats as stats\n",
        "\n",
        "# Filter the data to include only days with promotions and non-promotions\n",
        "promo_sales = df[df['Promo']==1]['Sales']\n",
        "nonpromo_sales = df[df['Promo']==0]['Sales']\n",
        "\n",
        "# Test for difference in means using two-sample t-test\n",
        "t, p = stats.ttest_ind(promo_sales, nonpromo_sales, equal_var=False)\n",
        "print('t-value: {:.2f}, p-value: {:.4f}'.format(t, p))"
      ],
      "metadata": {
        "id": "taxkHKhfS8k5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "t-value: 356.64 and p-value : 0.00 ,since p-value is less than significance value 0.05 we reject the null hypothesis.\n",
        "\n",
        "This concludes promotions are infact plays vital role in increase in sales."
      ],
      "metadata": {
        "id": "tboVWn7eIkx0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2)The effect of competition on sales: We are testing whether there is a significant correlation between the distance to the nearest competitor and sales**\n",
        "\n",
        "Null hypothesis: There is no significant relationship between competition distance and store sales.\n",
        "\n",
        "Alternative hypothesis: Stores located closer to competitors have significantly lower sales than stores located farther away."
      ],
      "metadata": {
        "id": "ns1MGbbaVKKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Calculate the Pearson correlation coefficient and p-value between sales and competition distance\n",
        "corr, p = pearsonr(df['Sales'], df['CompetitionDistance'])\n",
        "print('Correlation coefficient: {:.2f}, p-value: {:.4f}'.format(corr, p))"
      ],
      "metadata": {
        "id": "DCakZnanPOeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlation coefficient: -0.04, p-value: 0.0000 , since p-value is less than significance value 0.05 we reject the null hypothesis.\n",
        "\n",
        "Hence we conclude that,Stores located closer to competitors have significantly lower sales than stores located farther away.\n"
      ],
      "metadata": {
        "id": "1Do_Rhr8JYev"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3)The effect of store type on sales: You could test whether there is a significant difference in sales between different types of stores.**\n",
        "\n",
        "Null hypothesis: There is no significant difference in sales between different store types.\n",
        "\n",
        "Alternative hypothesis: Some store types have significantly higher sales than others."
      ],
      "metadata": {
        "id": "YN3rOm0jUvdv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import f_oneway\n",
        "\n",
        "# Filter the data to include only the three store types\n",
        "store_a_sales = df[df['StoreType']=='a']['Sales']\n",
        "store_b_sales = df[df['StoreType']=='b']['Sales']\n",
        "store_c_sales = df[df['StoreType']=='c']['Sales']\n",
        "\n",
        "# Test for difference in means using one-way ANOVA\n",
        "f, p = f_oneway(store_a_sales, store_b_sales, store_c_sales)\n",
        "print('F-value: {:.2f}, p-value: {:.4f}'.format(f, p))"
      ],
      "metadata": {
        "id": "j6BTi1GUVkNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "F-value: 7723.02, p-value: 0.0000 since p-value is less than significance value 0.05 we reject the null hypothesis.\n",
        "\n",
        "we can also verify from the graph that store type has higher sales than other store types."
      ],
      "metadata": {
        "id": "w4X890yzMfh9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To start our analysis, we imported two datasets that contain information on sales and stores. We then merged these datasets to create a single, comprehensive dataframe.\n",
        "\n",
        "Next, we examined the relationship between customers and other variables in the dataframe. Our analysis revealed that there is a positive correlation between the number of customers and sales, and that promotions have a slight effect on increasing customer numbers. We also observed that customers tend to do more shopping on state holidays, and that the impact of school holidays on customer behavior is minimal.\n",
        "\n",
        "After examining customer behavior, we investigated the relationship between sales and other variables in the dataframe. Our analysis showed that sales tend to be higher when store owners promote their shops and during state holidays. Additionally, we found that StoreType b and Assortment Type c tend to yield higher sales and profits. There was also a gradual decrease in mean sales as new competition entered the market.\n",
        "\n",
        "We conducted three hypothesis tests to determine if sales were affected by promotions, competition distance, and store type. In all cases, we rejected the null hypothesis.\n",
        "\n",
        "Our findings suggest that continuing to run promotions after the initial launch can have a negative effect on sales, and that promoting stores from January to October can increase sales. We also noted that mean sales have increased over the years, with the highest sales occurring in October, November, and December.\n",
        "\n",
        "In conclusion, our analysis provides valuable insights into the factors that affect sales and customer behavior at Rossmann stores. By understanding these relationships, store owners can make data-driven decisions to improve their profitability and overall success.  "
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### feature manipulation"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "df.columns"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = df.copy()"
      ],
      "metadata": {
        "id": "vgeicBkScreB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert CompetitionOpenSinceMonth column to integer data type\n",
        "df1['CompetitionOpenSinceMonth'] = df1['CompetitionOpenSinceMonth'].astype(int)\n",
        "\n",
        "# Convert CompetitionOpenSinceYear column to integer data type\n",
        "df1['CompetitionOpenSinceYear'] = df1['CompetitionOpenSinceYear'].astype(int)"
      ],
      "metadata": {
        "id": "uB283QKEcwsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#changing promo2 features into meaningful inputs\n",
        "#combining promo2 to total months\n",
        "df1['Promo2Open'] = (df1['Year'] - df1['Promo2SinceYear'])*12 + (df1['WeekOfYear'] - df1['Promo2SinceWeek'])*0.230137\n",
        "\n",
        "#correcting the neg values\n",
        "df1['Promo2Open'] = df1['Promo2Open'].apply(lambda x:0 if x < 0 else x)*df1['Promo2']\n",
        "\n",
        "#creating a feature for promo interval and checking if promo2 was running in the sale month\n",
        "def promo2running(df):\n",
        "  month_dict = {1:'Jan', 2:'Feb', 3:'Mar', 4:'Apr', 5:'May', 6:'Jun', 7:'Jul', 8:'Aug', 9:'Sept', 10:'Oct', 11:'Nov', 12:'Dec'}\n",
        "  try:\n",
        "    months = df1['PromoInterval'].split(',')\n",
        "    if df1['Month'] and month_dict[df1['Month']] in months:\n",
        "      return 1\n",
        "    else:\n",
        "      return 0\n",
        "  except Exception:\n",
        "    return 0\n",
        "\n",
        "#Applying \n",
        "df1['Promo2Open'] = df1.apply(promo2running,axis=1)*df1['Promo2']\n",
        "\n",
        "#Dropping unecessary columns\n",
        "df1.drop(['Promo2SinceYear','Promo2SinceWeek'],axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "oLdhDdOodIBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the bin edges and labels\n",
        "bin_edges = [0, 500, 1500, 3000, 5000, np.inf]\n",
        "bin_labels = ['near', 'medium', 'far', 'very far', 'extreme']\n",
        "\n",
        "# Create the CompetitionDistanceGroup column\n",
        "df1['CompetitionDistanceGroup'] = pd.cut(df1['CompetitionDistance'], bins=bin_edges, labels=bin_labels)\n",
        "\n",
        "# Show the first 5 rows of the new column\n",
        "print(df1[['CompetitionDistance', 'CompetitionDistanceGroup']].head())"
      ],
      "metadata": {
        "id": "lTPMSCY-dJHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This will create a new column in the dataframe called 'AvgSalesPerCustomer', which will contain the average sales per customer for each store.\n",
        "df1['AvgSalesPerCustomer'] = df1['Sales'] / df1['Customers']"
      ],
      "metadata": {
        "id": "jccf6vQcdX8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill missing values in the AvgSalesPerCustomer column with the mean\n",
        "df1['AvgSalesPerCustomer'].fillna(df1['AvgSalesPerCustomer'].mean(),inplace = True)"
      ],
      "metadata": {
        "id": "DR_KAkz1dddg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ENCODING"
      ],
      "metadata": {
        "id": "HkVG1Yswe980"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating variable which stores feature names.\n",
        "X_features = [ 'Store', 'DayOfWeek', 'Sales', 'Customers', 'Open', 'Promo',\n",
        "       'StateHoliday', 'SchoolHoliday', 'StoreType', 'Assortment',\n",
        "        'CompetitionOpenSinceMonth','CompetitionOpenSinceYear', 'Promo2', 'PromoInterval', 'Year', 'Month',\n",
        "       'WeekOfYear', 'DayOfYear', 'Promo2Open', 'CompetitionDistanceGroup',\n",
        "       'AvgSalesPerCustomer'\n",
        "          ]"
      ],
      "metadata": {
        "id": "faV_V1PLuufT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the categorical features to be one-hot encoded\n",
        "categorical_features = ['StoreType', 'Assortment', 'PromoInterval', 'CompetitionDistanceGroup']\n",
        "\n",
        "# Use Pandas get_dummies() function to perform one-hot encoding on the selected categorical features\n",
        "encoded_df = pd.get_dummies(df1[X_features], columns=categorical_features, drop_first=True)\n",
        "\n",
        "# The encoded_df DataFrame now has one-hot encoded columns for each of the selected categorical features\n",
        "encoded_df.head()"
      ],
      "metadata": {
        "id": "p1itHyehAQAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " The categorical data needs to first be converted to numerical data. **One-hot encoding** is one of the techniques used to perform this conversion. This method is mostly used when deep learning techniques are to be applied to sequential classification problems.\n",
        "\n",
        "One-hot encoding is essentially the representation of categorical variables as binary vectors. These categorical values are first mapped to integer values. Each integer value is then represented as a binary vector that is all 0s (except the index of the integer which is marked as 1).\n",
        "\n",
        "Many machine learning algorithms cannot operate on label data directly. They require all input variables and output variables to be numeric.\n",
        "\n",
        "In general, this is mostly a constraint of the efficient implementation of machine learning algorithms rather than hard limitations on the algorithms themselves.\n",
        "\n",
        "This means that categorical data must be converted to a numerical form. If the categorical variable is an output variable, we may also want to convert predictions by the model back into a categorical form in order to present them or use them in some application."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = encoded_df.copy()"
      ],
      "metadata": {
        "id": "5pPceskWRaXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Storing feature names in index variable.\n",
        "index = ['Store', 'DayOfWeek','Customers', 'Open', 'Promo',\n",
        "       'StateHoliday', 'SchoolHoliday', 'CompetitionOpenSinceMonth',\n",
        "       'CompetitionOpenSinceYear', 'Promo2', 'Year', 'Month', 'WeekOfYear',\n",
        "       'DayOfYear', 'Promo2Open', 'AvgSalesPerCustomer', 'StoreType_b',\n",
        "       'StoreType_c', 'StoreType_d', 'Assortment_b', 'Assortment_c',\n",
        "       'PromoInterval_Feb,May,Aug,Nov', 'PromoInterval_Jan,Apr,Jul,Oct',\n",
        "       'PromoInterval_Mar,Jun,Sept,Dec', 'CompetitionDistanceGroup_medium',\n",
        "       'CompetitionDistanceGroup_far', 'CompetitionDistanceGroup_very far',\n",
        "       'CompetitionDistanceGroup_extreme']"
      ],
      "metadata": {
        "id": "d5yC43UainoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a constant term to the feature matrix for the intercept\n",
        "X = sm.add_constant(df2[index])\n",
        "\n",
        "# Set the target variable\n",
        "Y = df2['Sales']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n"
      ],
      "metadata": {
        "id": "Hw9QL7DQ1Asj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit OLS regression model\n",
        "model_1 = sm.OLS(y_train, X_train).fit()\n",
        "\n",
        "# Print summary of model results\n",
        "model_1.summary2()"
      ],
      "metadata": {
        "id": "RPVkGHDl1wBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**INSIGHTS**"
      ],
      "metadata": {
        "id": "KhaSlv2OjagU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As per the table , all variables have p < 0.05 value except CompetitionOpenSinceYear,month and WeekOfYear and are statistically insignificant.The model says that these three variables are not influencing the 'Sales' column at a significance level of 0.05."
      ],
      "metadata": {
        "id": "8b7jnKAmjQZn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**HANDLING MULTI-COLLINEARITY**"
      ],
      "metadata": {
        "id": "L6OnpN1E-s9j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#function to VIF of every variable\n",
        "def get_vif_factors(X):\n",
        "  #converting to dataframe to a matrix\n",
        "  X_matrix = X.to_numpy()\n",
        "  #Using list comprehension to store vif of each variable \n",
        "  vif = [ variance_inflation_factor(X_matrix,i) for i in range(X.shape[1])]\n",
        "  #Creating an empty dataframe.\n",
        "  vif_factors = pd.DataFrame()\n",
        "  #Storing columns name \n",
        "  vif_factors['column'] = X.columns\n",
        "  #Storing corresponding vifs \n",
        "  vif_factors['vif'] = vif\n",
        "\n",
        "  return vif_factors"
      ],
      "metadata": {
        "id": "TDfSac-k-1cX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calling get_vif_factors function\n",
        "vif_factors = get_vif_factors(df2[index])\n",
        "vif_factors"
      ],
      "metadata": {
        "id": "oEC1EGuB-sNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CHECKING CORRELATION OF COLUMNS WITH LARGE VIFs**"
      ],
      "metadata": {
        "id": "Z5tJP7sMA8fw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Storing column names which have vif value more than four\n",
        "columns_with_large_vif =['PromoInterval_Feb,May,Aug,Nov','PromoInterval_Jan,Apr,Jul,Oct','PromoInterval_Mar,Jun,Sept,Dec']"
      ],
      "metadata": {
        "id": "04SQiicvA7CI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then plotting the heatmap for features with VIF more than 4"
      ],
      "metadata": {
        "id": "01_x-TtgB884"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (12,10))\n",
        "sns.heatmap(df2[columns_with_large_vif].corr(),annot = True)\n",
        "plt.title(\" Heatmap depicting correlation between features\")"
      ],
      "metadata": {
        "id": "rrn1lNhSB8lY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**INSIGHTS**"
      ],
      "metadata": {
        "id": "nGz8PDX3lJ2u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No multicollinearity detected "
      ],
      "metadata": {
        "id": "zg6ZbmBdlM-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Storing varibles except CompetitionOpenSinceYear,month and WeekOfYear\n",
        "index2 = ['Store', 'DayOfWeek', 'Customers', 'Open', 'Promo', 'StateHoliday',\n",
        "       'SchoolHoliday', 'CompetitionOpenSinceMonth',\n",
        "       'Promo2', 'Year',\n",
        "       'DayOfYear', 'Promo2Open', 'AvgSalesPerCustomer', 'StoreType_b',\n",
        "       'StoreType_c', 'StoreType_d', 'Assortment_b', 'Assortment_c',\n",
        "       'PromoInterval_Feb,May,Aug,Nov', 'PromoInterval_Jan,Apr,Jul,Oct',\n",
        "       'PromoInterval_Mar,Jun,Sept,Dec', 'CompetitionDistanceGroup_medium',\n",
        "       'CompetitionDistanceGroup_far', 'CompetitionDistanceGroup_very far',\n",
        "       'CompetitionDistanceGroup_extreme']"
      ],
      "metadata": {
        "id": "jMS_tbS8l9QV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filtering out statistically insignificant variables and creating new training set.\n",
        "x_train = X_train[index2]\n",
        "\n",
        "#training the model\n",
        "model_2 = sm.OLS(y_train,x_train).fit()\n",
        "\n",
        "#summary\n",
        "model_2.summary2()"
      ],
      "metadata": {
        "id": "CKQaA3cIE1jo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**INSIGHTS**"
      ],
      "metadata": {
        "id": "1W8J0bS5sJPs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All variables are statistically significant since p-value is  < 0.05"
      ],
      "metadata": {
        "id": "Gw1KCm6ore9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To improve the accuracy of a machine learning model, it is important to identify any predictor variables that may not have a significant impact on the target variable, which in your case is the 'Sales' column. One way to do this is by examining the model summary and identifying variables with high p-values.\n",
        "\n",
        "Another important consideration is multicollinearity, which occurs when predictor variables are highly correlated with one another. To check for multicollinearity, we calculated the Variance Inflation Factor (VIF) for each variable. A high VIF value, generally considered to be greater than 4, suggests that a variable may be contributing to multicollinearity. To investigate these variables further, we created a heatmap to visualize their relationships with other variables in the model. This helped us gain a better understanding of which variables may need to be modified or removed to improve model performance. "
      ],
      "metadata": {
        "id": "DRyyqDemtAOm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "s per the table , all variables have p < 0.05 and are statistically significant columns CompetitionOpenSinceYear,month and WeekOfYear are statistically insignificant.The model says that these three variables are not influencing the 'Sales' column at a significance level of 0.05.."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RESIDUAL ANALYSIS**"
      ],
      "metadata": {
        "id": "pHgPX4yQN4-D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test for Normality of Residuals(P-P plot)"
      ],
      "metadata": {
        "id": "p4AX7g1OPr9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_pp_plot(model, title):\n",
        "    # Create a probability plot object for the model residuals\n",
        "    probplot = sm.ProbPlot(model.resid)\n",
        "    \n",
        "    # Set the size of the plot\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    \n",
        "    # Create a probability plot (pp) plot of the model residuals\n",
        "    # with a line at 45 degrees (representing a normal distribution)\n",
        "    probplot.ppplot(line='45')\n",
        "    \n",
        "    # Set the plot title\n",
        "    plt.title(title)\n",
        "    \n",
        "    # Show the plot\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "K5j4Cq7od0on"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calling draw_pp_plot function \n",
        "draw_pp_plot(model_2,\"Normal P-P Plot of Regression Standardized Residuals\")"
      ],
      "metadata": {
        "id": "YJYH6kNuPBcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reason for chosing pp plot**The probability plot (pp plot) is a graphical method used to assess the normality of a distribution. In a pp plot, the quantiles of the sample distribution are plotted against the corresponding quantiles of a theoretical normal distribution. If the two distributions are similar, the points on the plot will fall along a straight line at a 45-degree angle.\n",
        "\n",
        "In the case of a linear regression model, the residuals represent the difference between the observed values of the dependent variable and the predicted values from the model. If the residuals are normally distributed, it indicates that the model has captured the underlying patterns in the data and that the assumptions of linear regression are satisfied."
      ],
      "metadata": {
        "id": "s4evjK3TwsN2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**INSIGHTS**"
      ],
      "metadata": {
        "id": "bMHdJVlHy5bG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The points on the plot deviate significantly from the 45-degree line, it suggests that the residuals may not be normally distributed."
      ],
      "metadata": {
        "id": "ZaBO11-3xKvw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_standardized_values(vals):\n",
        "    # Calculate the mean of the input values\n",
        "    mean = vals.mean()\n",
        "    \n",
        "    # Calculate the standard deviation of the input values\n",
        "    std_dev = vals.std()\n",
        "    \n",
        "    # Calculate the standardized values of the input values by\n",
        "    # subtracting the mean and dividing by the standard deviation\n",
        "    standardized_vals = (vals - mean) / std_dev\n",
        "    \n",
        "    # Return the standardized values\n",
        "    return standardized_vals"
      ],
      "metadata": {
        "id": "XXBL3PnSRkN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_resid_fitted(fitted, resid, title):\n",
        "    # Calculate the standardized predicted values by calling the get_standardized_values() function on the fitted values\n",
        "    standardized_fitted = get_standardized_values(fitted)\n",
        "    \n",
        "    # Calculate the standardized residuals by calling the get_standardized_values() function on the residual values\n",
        "    standardized_resid = get_standardized_values(resid)\n",
        "    \n",
        "    # Create a scatter plot of the standardized residuals against the standardized predicted values\n",
        "    plt.scatter(standardized_fitted, standardized_resid)\n",
        "    \n",
        "    # Set the plot title\n",
        "    plt.title(title)\n",
        "    \n",
        "    # Set the x-axis label\n",
        "    plt.xlabel(\"Standardized predicted values\")\n",
        "    \n",
        "    # Set the y-axis label\n",
        "    plt.ylabel(\"Standardized residuals values\")\n",
        "    \n",
        "    # Show the plot\n",
        "    plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MJxIygjkP3K7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Residual Plot for Homoscedasticity and Model Specification "
      ],
      "metadata": {
        "id": "rSMQEx8dQN1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_resid_fitted(model_2.fittedvalues,model_2.resid,\"Residual Plot\")"
      ],
      "metadata": {
        "id": "iotuY-1wRFeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reason for chosing scatter plot**The plot created by the plot_resid_fitted() function, which is a scatter plot of standardized residuals against standardized predicted values, is a useful tool for evaluating the fit of a linear regression model.\n",
        "The standardized values are used for plotting so that the values are comparable and easier to interpret. The scatter plot can help you identify any patterns or outliers in the data and assess whether the assumptions of the linear regression model are met. If there is a discernible pattern in the plot, it may indicate that the assumptions of the linear regression model have been violated, and further investigation may be necessary."
      ],
      "metadata": {
        "id": "Slpeo8CwzqNn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**INSIGHTS**"
      ],
      "metadata": {
        "id": "ALt9qRth06Av"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "there is a discernible pattern or trend in the plot, it suggests that the model has not captured all of the relevant information in the data and may not be a good fit."
      ],
      "metadata": {
        "id": "52HSULJr1DeX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The points on the plot created by plot_resid_fitted() deviate significantly from the 45-degree line and there is a discernible pattern or trend in the residual plot for homoscedasticity and model specification, it could indicate that the assumptions of the linear regression model have been violated.\n",
        "Data transformation is a possible solution.\n",
        "\n",
        "I have decide to use a square root transformation for the target variable  Here are some reasons why it could be a good idea:\n",
        "\n",
        "Skewed data: If the distribution of the target variable is skewed, taking the square root can help to normalize the distribution, which can make it easier to model using linear regression.\n",
        "\n",
        "Heteroscedasticity: If the variance of the target variable increases or decreases as its mean value changes, it can violate the assumption of homoscedasticity in linear regression. Taking the square root can help to stabilize the variance and make the data more homoscedastic.\n",
        "\n",
        "Interpretation: If the research question involves interpreting the effect of the independent variables on a percentage change in the target variable, taking the square root can be useful. This is because a square root transformation results in a percentage change in the target variable that is proportional to the original value.\n",
        "\n",
        "Outliers: If the data contains outliers that have a disproportionate effect on the model, taking the square root can help to reduce the influence of these extreme values.\n",
        "\n",
        "In conclusion, using a square root transformation for the target variable can be a good idea if it addresses issues with skewed data, heteroscedasticity, interpretation, or outliers."
      ],
      "metadata": {
        "id": "1gfB5-FV2W6S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "y_train = np.sqrt(y_train)"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fitting the model\n",
        "model_3 = sm.OLS(y_train,x_train).fit()\n"
      ],
      "metadata": {
        "id": "TbNOQ00yT7M6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "draw_pp_plot(model_3,\"Normal P-P Plot of Regression Standardized Residuals\")"
      ],
      "metadata": {
        "id": "ftN1FFiOUkG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reason for chosing pp plot**The probability plot (pp plot) is a graphical method used to assess the normality of a distribution. In a pp plot, the quantiles of the sample distribution are plotted against the corresponding quantiles of a theoretical normal distribution. If the two distributions are similar, the points on the plot will fall along a straight line at a 45-degree angle.\n",
        "\n",
        "In the case of a linear regression model, the residuals represent the difference between the observed values of the dependent variable and the predicted values from the model. If the residuals are normally distributed, it indicates that the model has captured the underlying patterns in the data and that the assumptions of linear regression are satisfied."
      ],
      "metadata": {
        "id": "11qz19vJ4Afx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**INSIGHTS**"
      ],
      "metadata": {
        "id": "IFNz54Uk3623"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The points on the plot does not deviate significantly from the 45-degree line, it suggests that the residuals is somewhat normally distributed now."
      ],
      "metadata": {
        "id": "JDlFcxny4Gqx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#plotting distribution plot for targeted value\n",
        "sns.distplot(x=df2['Sales'])"
      ],
      "metadata": {
        "id": "WNZ3nrlr4t0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reason for chosing distribution plot:** for checking the distribution of target variable ."
      ],
      "metadata": {
        "id": "ncAnfhh649sA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**INSIGHTS**"
      ],
      "metadata": {
        "id": "pnVNX-w-5LbX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "it seems from the graph that target variable distribution is right skewed.Hence our analysis was correct, we need to transform our data."
      ],
      "metadata": {
        "id": "FoWmMnHI5SxS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Transforming the data by taking the square root of the variable\n",
        "df2['Sales'] = np.sqrt(df2['Sales'])"
      ],
      "metadata": {
        "id": "OpdSRBJIqBb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.distplot(x=df2['Sales'])"
      ],
      "metadata": {
        "id": "Yi9jSPATrQZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**INSIGHTS** : Now the data is normally distributed. A normally distributed target variable is important in linear regression to ensure the validity of statistical tests, accurate parameter estimation, accurate predictions, and reliable model interpretation. \n"
      ],
      "metadata": {
        "id": "1WZiEziG57aC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **OUTLIER DETECTION**"
      ],
      "metadata": {
        "id": "WqMVvUPwrlMr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t2p6M6UurrLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df3 = df2.copy()"
      ],
      "metadata": {
        "id": "-Jq2QSPqsSsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the z-score of Sales column\n",
        "df3['zscore'] = zscore(df3['Sales'])\n",
        "\n",
        "# Get the rows where the z-score is greater than 3 or less than -3\n",
        "outliers = df3[(df3['zscore']>3.0) | (df3['zscore']<-3)]\n",
        "\n",
        "# Remove the outliers from the original dataframe\n",
        "df_no_outliers = df3[~((df3['zscore']>3.0) | (df3['zscore']<-3))]\n",
        "\n",
        "# Remove the z-score column from the cleaned dataframe\n",
        "df4 = df_no_outliers.drop('zscore', axis=1)"
      ],
      "metadata": {
        "id": "NpzhVlU8rk3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "which outlier detection system has been used and why?"
      ],
      "metadata": {
        "id": "2LjiWrr8Cn-L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the given code, z-score method has been used for outlier detection. Z-score method helps in identifying the outliers by measuring the deviation of a particular data point from the mean of a group of data points and scaling it by the standard deviation of the group. The rows having a z-score greater than 3 or less than -3 are considered as outliers and are removed from the original dataframe.\n",
        "\n",
        "Z-score method is one of the commonly used methods for outlier detection, as it is easy to implement and understand. However, it assumes the data to be normally distributed, which may not always be the case in real-world scenarios. Therefore, other outlier detection techniques such as IQR (Interquartile Range) or Local Outlier Factor may also be used, depending on the nature and characteristics of the data."
      ],
      "metadata": {
        "id": "_IoJG0LmCmxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#The outliers rows\n",
        "outliers"
      ],
      "metadata": {
        "id": "3SMv00VFskpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Initializing the StandardScaler\n",
        "X_scaler = StandardScaler()\n",
        "#Standardizie all the feature columns \n",
        "X_scaled = X_scaler.fit_transform(df4[index2])\n",
        "\n",
        "#Standardizing Y by explicitly by substracting mean and divding by standard deviation\n",
        "Y = (df4['Sales']-df4['Sales'].mean())/df4['Sales'].std()"
      ],
      "metadata": {
        "id": "v-Qwemp8qm-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The given code uses two different methods to standardize the data:\n",
        "\n",
        "StandardScaler: This method is used to standardize the feature columns in df4[index2]. StandardScaler scales each feature column so that it has a mean of 0 and a standard deviation of 1. This method is commonly used for standardizing features in machine learning models, as it helps to ensure that all features are on a similar scale and avoids bias towards features with larger values.\n",
        "\n",
        "Explicit scaling: This method is used to standardize the target variable Sales. It subtracts the mean of the Sales column from each value and divides by the standard deviation.The explicit scaling method is useful for standardizing target variables or other data that is not in a dataframe format."
      ],
      "metadata": {
        "id": "BIi4zXf99Lgi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have already performed multicollinearity check and feature selection,we may not need dimensionality reduction. This is because the aim of feature selection is to identify and keep the most informative features, while removing the redundant or irrelevant ones. Multicollinearity check ensures that the features are not highly correlated with each other, which can lead to overfitting and instability of the model. By performing these steps, we have already reduced the dimensionality of the data to a set of non-redundant features, which are the most important in explaining the target variable.\n",
        "\n",
        "Moreover, dimensionality reduction techniques like PCA are usually used when the data has a large number of features that are highly correlated or where there are many features with similar importance. In our case, we have already removed the features that are highly correlated and are left with a set of non-redundant features. Therefore, applying PCA may not provide significant improvement in model performance, and may even result in a loss of interpretability of the model.\n",
        "\n",
        "In summary, we have already performed multicollinearity check and feature selection, we may not need dimensionality reduction as you have already reduced the dimensionality of the data to a set of non-redundant and informative features, which are sufficient for modeling the target variable."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "x_train,x_test,y_train,y_test = train_test_split(X_scaled,Y,test_size = 0.2,random_state = 42)"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why? "
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the given code, the data has been split into training and testing sets using a splitting ratio of 0.2, which means 20% of the data is kept aside for testing and the remaining 80% is used for training the model.\n",
        "\n",
        "The choice of splitting ratio depends on the size of the dataset and the problem at hand. In general, a larger ratio of training to testing data is preferred when the dataset is large, as this allows the model to be trained on a more diverse range of examples and can lead to better performance.\n",
        "\n",
        "However, if the dataset is relatively small, a larger ratio of testing to training data is preferred to ensure that the model is evaluated on a sufficient number of examples and that the evaluation is representative of the generalization performance of the model.\n",
        "\n",
        "In this case, a 20% ratio for testing data has been chosen, which is a common ratio used in many machine learning applications. The choice of 20% allows for a large enough test set to evaluate the model's performance, while still leaving a sufficiently large training set to train the model. The random state of 42 is also chosen to ensure that the data is split in a consistent manner across multiple runs of the code."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this Rossman retail stores dataset, the objective is to predict the amount of sales, which is a continuous variable. While there are multiple stores, the data is not inherently imbalanced since the sales are not binary or categorical values that can lead to an unequal distribution of classes. Instead, sales values vary continuously and are spread out across the dataset.\n",
        "\n",
        "Furthermore, the dataset is composed of over a million observations across different stores, so there is a large sample size to work with. This large sample size helps to reduce the impact of any outliers or rare occurrences that could skew the data in one direction or another. Additionally, the dataset includes information about the stores' features, such as the number of competitors, holidays, and promotions, which can be useful in creating a well-informed model and reducing the impact of data imbalance.\n",
        "\n",
        "Overall, data imbalance is not a major issue in this dataset since the sales values are continuous and the dataset is composed of a large number of observations, which helps to reduce the impact of any outliers or rare occurrences. With the additional store features provided, a well-informed model can be created that can accurately predict the amount of sales for different stores."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def calculate_metrics(model_name, model, x_train, x_test, y_train, y_test):\n",
        "   \n",
        "    # Make predictions on the training and test sets\n",
        "    y_train_pred = model.predict(x_train)\n",
        "    y_test_pred = model.predict(x_test)\n",
        "\n",
        "    # Calculate the metrics\n",
        "    train_mae = mean_absolute_error(y_train, y_train_pred)\n",
        "    train_mse = mean_squared_error(y_train, y_train_pred)\n",
        "    train_rmse = mean_squared_error(y_train, y_train_pred, squared=False)\n",
        "    train_r2 = r2_score(y_train, y_train_pred)\n",
        "    n = len(y_train)\n",
        "    k = x_train.shape[1]  # number of independent variables\n",
        "    train_adj_r2 = 1 - ((1 - train_r2) * (n - 1) / (n - k - 1))\n",
        "\n",
        "    test_mae = mean_absolute_error(y_test, y_test_pred)\n",
        "    test_mse = mean_squared_error(y_test, y_test_pred)\n",
        "    test_rmse = mean_squared_error(y_test, y_test_pred, squared=False)\n",
        "    test_r2 = r2_score(y_test, y_test_pred)\n",
        "    n = len(y_test)\n",
        "    k = x_test.shape[1]  # number of independent variables\n",
        "    test_adj_r2 = 1 - ((1 - test_r2) * (n - 1) / (n - k - 1))\n",
        "\n",
        "    data = {\n",
        "        'Model_Name': [model_name],\n",
        "        'Train_MAE': [train_mae],\n",
        "        'Train_MSE': [train_mse],\n",
        "        'Train_RMSE': [train_rmse],\n",
        "        'Train_R2': [train_r2],\n",
        "        'Train_Adj_R2': [train_adj_r2],\n",
        "        'Test_MAE': [test_mae],\n",
        "        'Test_MSE': [test_mse],\n",
        "        'Test_RMSE': [test_rmse],\n",
        "        'Test_R2': [test_r2],\n",
        "        'Test_Adj_R2': [test_adj_r2]\n",
        "    }\n",
        "    df = pd.DataFrame(data)\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "3_pt7EAIfODc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ridge Regression model"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#initialising the model\n",
        "ridge = Ridge()\n",
        "#fitting the model\n",
        "ridge.fit(x_train,y_train)"
      ],
      "metadata": {
        "id": "0KoaTR7Tepa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ridge regression is a regularization technique used in linear regression models to prevent overfitting by adding a penalty term to the cost function. It does this by adding a regularization parameter, denoted as  (lambda), which controls the amount of shrinkage applied to the coefficients of the regression model.\n",
        "\n",
        "In ridge regression, the ordinary least squares (OLS) cost function is modified to include a penalty term that is proportional to the sum of the squared values of the regression coefficients. This penalty term imposes a constraint on the model that forces the coefficients to be small, which can reduce the variance of the model and prevent overfitting.\n",
        "\n",
        "Ridge regression is particularly useful when dealing with multicollinearity, which is the presence of strong correlations between predictor variables. When multicollinearity is present, the OLS estimator can have high variance, making it difficult to determine which predictors are important. By adding the regularization term to the cost function, ridge regression can help to reduce the variance of the coefficients, making it easier to identify important predictors and improve the accuracy of the model.\n"
      ],
      "metadata": {
        "id": "e5OFBKGujWi0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#printing coefficients\n",
        "ridge.coef_"
      ],
      "metadata": {
        "id": "BYA4Xo5ue8vq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculating matrix\n",
        "metrics_1 = calculate_metrics('ridge regression',ridge,x_train, x_test, y_train, y_test)"
      ],
      "metadata": {
        "id": "L_luB_1Kh34U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_1"
      ],
      "metadata": {
        "id": "U5LPDuHbCLpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the provided metrics, the Ridge regression model appears to be performing well on both the training and test data. The training and test mean absolute error (MAE), mean squared error (MSE), and root mean squared error (RMSE) values are all relatively close, indicating that the model is not overfitting to the training data. Additionally, the R-squared and adjusted R-squared values are both high on both the training and test data, indicating that the model is able to explain a significant amount of the variability in the target variable.\n",
        "\n",
        "Overall, the metrics suggest that the Ridge regression model is a good fit for the data and is able to accurately predict the sales amount."
      ],
      "metadata": {
        "id": "NhQvwygzoHrX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error, make_scorer"
      ],
      "metadata": {
        "id": "vH3FtTeoEZvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define a Ridge model object\n",
        "ridge1 = Ridge()\n",
        "#Define a dictionary of hyperparameter values to be tuned\n",
        "params = {\n",
        "    'alpha': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "    'normalize': [True, False]\n",
        "}"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ridge_cv = GridSearchCV(ridge1, params, cv=5)\n",
        "ridge_cv.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "4COv0D5-_jzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ridge_best = Ridge(**ridge_cv.best_params_)\n",
        "ridge_best.fit(X_scaled, Y)"
      ],
      "metadata": {
        "id": "8aRn8r-B_qO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#metrics evaluation\n",
        "metrics_2 = calculate_metrics('Ridge REgression',ridge_best,x_train, x_test, y_train, y_test)"
      ],
      "metadata": {
        "id": "oIrO6ZJ6_tjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_2"
      ],
      "metadata": {
        "id": "u0LNLkWgChgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cross-validation results seem to be very similar to the original model, with only very small differences in the performance metrics. This suggests that the model is fairly stable and robust, and is not overfitting the data. It is always a good idea to perform cross-validation to ensure that the model is not overfitting, and to obtain a more accurate estimate of the model's performance on new, unseen data. The fact that the cross-validation results are very similar to the original model suggests that the model is likely to perform well on new data, and that the performance metrics obtained are reliable and accurate."
      ],
      "metadata": {
        "id": "JyT2eFrDxtND"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The hyperparameter optimization technique used in this code is GridSearchCV.\n",
        "\n",
        "GridSearchCV is a method that performs an exhaustive search over specified hyperparameter values for an estimator. It evaluates a model performance with the specified hyperparameters using cross-validation and returns the hyperparameters that result in the best performance.\n",
        "\n",
        "This technique was chosen because it is a simple yet effective method for finding the optimal hyperparameters for a model. It allows for a systematic approach to hyperparameter tuning and can help avoid overfitting or underfitting of the model. Additionally, it can save time and effort by automating the search process."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Forest Regressor."
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest Regressor is a supervised learning algorithm that belongs to the family of ensemble methods. It is an extension of the decision tree algorithm that builds multiple decision trees and combines their predictions to obtain a more accurate and stable model.\n",
        "\n",
        "In the random forest algorithm, a set of decision trees are created using a random subset of the features and training data. During training, each decision tree is built on a different subset of the training data, and at each node of the tree, a random subset of features is used to find the best split. This randomization and aggregation process helps to reduce overfitting and improve the generalization of the model.\n",
        "\n",
        "To make a prediction for a new data point, the algorithm aggregates the predictions of all the decision trees in the forest. The final prediction is the mean or the median of the individual tree predictions.\n",
        "\n",
        "Random Forest Regressor is used for regression tasks and can handle both continuous and categorical data. It is a powerful algorithm that is widely used in data science and machine learning for its high accuracy, robustness, and ability to handle large datasets."
      ],
      "metadata": {
        "id": "pLCYQU9FPpxL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Initialize the Random Forest model\n",
        "#rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "# Fit the model on the training data\n",
        "#rf_model.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Storing preious saved data\n",
        "data = { \"Model_Name\":'RandomForestRegressor',\"Train_MAE\":0.000619,\"Train_MSE\":0.000005,\"Train_RMSE\":0.002225,\"Train_R2\":0.999995,\"Train_Adj_R2\": 0.999995,\"Test_MAE\":0.001475,\"Test_MSE\": 0.000041,\"Test_RMSE\": 0.006436,\"Test_R2\": 0.999958,\"Test_Adj_R2\":0.999908}"
      ],
      "metadata": {
        "id": "pF8ygvHrHzJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_3 = pd.DataFrame(data,index=[0])"
      ],
      "metadata": {
        "id": "5IlTsLdEMjR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_3"
      ],
      "metadata": {
        "id": "a3g70GHr8Dhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DISCLAIMER :** I have commented the code and save the results as it was taking too much time to evaluate and the cross validation part never got executed."
      ],
      "metadata": {
        "id": "m7O3GW50O0G6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Based on the evaluation metrics, it seems like the random forest regression model is performing very well on the training data, with very low values of MAE, MSE and RMSE and high values of R2 and Adjusted R2. The model is also performing well on the test data, with low values of MAE, MSE and RMSE and high values of R2 and Adjusted R2, although the values are slightly higher than the training data.\n",
        "\n",
        "These evaluation metrics suggest that the model is able to capture the relationships between the features and the target variable very well and is able to make accurate predictions on both the training and test data. However, it is important to note that the model may be overfitting the training data, as the evaluation metrics for the training data are significantly better than the evaluation metrics for the test data. Therefore, it may be necessary to further evaluate the model and potentially adjust the hyperparameters to reduce overfitting."
      ],
      "metadata": {
        "id": "sYvyLaH6Ocug"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Now we can create a RandomForestRegressor object and define a set of hyperparameters to tune\n",
        "#rf = RandomForestRegressor(random_state=42)\n",
        "#params = {\n",
        "#    'n_estimators': [10, 50, 100],\n",
        "#    'max_depth': [5, 10, None],\n",
        "#    'max_features': ['sqrt', 'log2', 0.5]\n",
        "#}\n",
        "\n",
        "#We'll use GridSearchCV to search over these hyperparameters to find the best set of hyperparameters. We'll specify the number of folds for cross-validation using the cv parameter\n",
        "#rf_cv = GridSearchCV(rf, params, cv=5)\n",
        "#rf_cv.fit(X_scaled, Y)\n",
        "\n",
        "#Now we can use the best hyperparameters to train a RandomForestRegressor model on the entire training set\n",
        "#rf_best = RandomForestRegressor(**rf_cv.best_params_, random_state=42)\n",
        "#rf_best.fit(X_scaled, Y)\n",
        "\n",
        "#metrics_4 = calculate_metrics(\"RandomForest_cross_validated\",rf_best,x_train, x_test, y_train, y_test)"
      ],
      "metadata": {
        "id": "Kdey7VRGNCEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The hyperparameter optimization technique used in this code is GridSearchCV.\n",
        "\n",
        "GridSearchCV is a method that performs an exhaustive search over specified hyperparameter values for an estimator. It evaluates a model performance with the specified hyperparameters using cross-validation and returns the hyperparameters that result in the best performance.\n",
        "\n",
        "This technique was chosen because it is a simple yet effective method for finding the optimal hyperparameters for a model. It allows for a systematic approach to hyperparameter tuning and can help avoid overfitting or underfitting of the model. Additionally, it can save time and effort by automating the search process."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lasso Regression"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating an object for Lasso\n",
        "lasso = Lasso()\n",
        "#Fitting the model\n",
        "lasso.fit(x_train,y_train)"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lasso coefficients\n",
        "lasso.coef_"
      ],
      "metadata": {
        "id": "kla43XKEpEgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluating metrics\n",
        "metrics_5 = calculate_metrics('Lasso regression',lasso,x_train, x_test, y_train, y_test)"
      ],
      "metadata": {
        "id": "0IroOxsnpceU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_5"
      ],
      "metadata": {
        "id": "3JzKr_oTpotI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The machine learning model used in this case is Lasso regression, which is a type of linear regression that uses regularization to prevent overfitting. The model has a poor performance, as indicated by the evaluation metric scores. The train R-squared score is 0.0, which means that the model does not explain any of the variation in the target variable. The adjusted R-squared score is negative, indicating that the model is a poor fit for the data. The train MAE, MSE, and RMSE scores are also high, indicating that the model's predictions are far from the true values.\n",
        "\n",
        "The test set scores are also poor, with negative R-squared and adjusted R-squared scores, indicating that the model's performance on new data is not good. The MAE, MSE, and RMSE scores are also high on the test set, indicating that the model is not generalizing well. Overall, the Lasso regression model is not a good fit for this data and may require more complex models or data preprocessing to improve performance."
      ],
      "metadata": {
        "id": "dChFprKEQRRX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#initialising a lasso object\n",
        "lasso = Lasso()\n",
        "#Define a dictionary of hyperparameter values to be tuned\n",
        "params = {\n",
        "    'alpha': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "    'normalize': [True, False]\n",
        "}"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#GridSearchCV object for Lasso regression with 5-fold cross-validation\n",
        "lasso_cv = GridSearchCV(lasso, params, cv=5)\n",
        "#Fitting the Lasso model on training data\n",
        "lasso_cv.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "lDetkb-nqG82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Initializing Lasso model with the best parameters obtained from GridSearchCV\n",
        "lasso_best = Lasso(**lasso_cv.best_params_)\n",
        "#Fitting the Lasso model on the scaled training data and target variable\n",
        "lasso_best.fit(X_scaled,Y)"
      ],
      "metadata": {
        "id": "5x3gFzivqgdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_6 = calculate_metrics('Lasso REgression',lasso_best,x_train, x_test, y_train, y_test)"
      ],
      "metadata": {
        "id": "Kx-TavaVqoOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_6"
      ],
      "metadata": {
        "id": "dMRV6L05qz9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The initial Lasso Regression model has high mean absolute error (MAE), mean squared error (MSE) and root mean squared error (RMSE) values, indicating poor performance. The R2 and adjusted R2 values for both the training and test sets are also close to zero, suggesting that the model does not explain much of the variance in the data.\n",
        "\n",
        "After cross-validation, there is a significant improvement in the performance of the Lasso Regression model. The MAE, MSE, and RMSE values have decreased considerably, indicating a better fit of the model to the data. The R2 and adjusted R2 values for the training and test sets have also improved, suggesting that the model now explains more of the variance in the data.\n",
        "\n",
        "Overall, the cross-validated Lasso Regression model appears to be a better choice than the initial model, as it has lower error values and better R2 scores."
      ],
      "metadata": {
        "id": "dy1Stu7nQ0jH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The hyperparameter optimization technique used in this code is GridSearchCV.\n",
        "\n",
        "GridSearchCV is a method that performs an exhaustive search over specified hyperparameter values for an estimator. It evaluates a model performance with the specified hyperparameters using cross-validation and returns the hyperparameters that result in the best performance.\n",
        "\n",
        "This technique was chosen because it is a simple yet effective method for finding the optimal hyperparameters for a model. It allows for a systematic approach to hyperparameter tuning and can help avoid overfitting or underfitting of the model. Additionally, it can save time and effort by automating the search process."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Gradient Boosting Regressor**"
      ],
      "metadata": {
        "id": "_0muG2mz4h0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Gradient Boosting Regressor\n",
        "gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "\n",
        "# Fit the model on the training data\n",
        "gbr.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "R8WypaEQwKuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient Boosting Regressor is a type of machine learning algorithm used for regression problems. It is an ensemble method that combines multiple weak models to create a stronger model.\n",
        "\n",
        "The algorithm works by sequentially adding weak models to the ensemble and adjusting the weights of each data point based on the error of the previous model. This allows the model to focus on the data points that were previously misclassified and improve its overall performance."
      ],
      "metadata": {
        "id": "bqls28SLrS8Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluating metrics\n",
        "metrics_7 = calculate_metrics('Gradient Boosting Regressor',gbr,x_train, x_test, y_train, y_test)\n",
        "metrics_7 "
      ],
      "metadata": {
        "id": "Ce2LGiIx0pcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model achieved a low MAE on both the training and testing data, indicating that it was able to predict sales values with relatively small errors. The MSE and RMSE were also low, further indicating the model's ability to accurately predict sales.\n",
        "\n",
        "The R-squared values were close to 1 on both the training and testing data, indicating that the model explains a large proportion of the variability in the data. The adjusted R-squared values were also high, suggesting that the model is not overfitting to the training data.\n",
        "\n",
        "Overall, the results suggest that the Gradient Boosting Regressor model is an effective approach for predicting sales in Rossmann stores, and that the model is able to generalize well to new data."
      ],
      "metadata": {
        "id": "vFhjVk0_rGEM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CROSS VALIDATION AND HYPERPARAMETER TUNNING**"
      ],
      "metadata": {
        "id": "GHBfVc_t3-V-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "gbr = GradientBoostingRegressor(random_state=42)\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 500],\n",
        "    'learning_rate': [0.01, 0.1, 1],\n",
        "    'max_depth': [3, 5, 7]\n",
        "}\n",
        "grid_search = GridSearchCV(gbr, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "grid_search.fit(X_scaled,Y)\n",
        "\n",
        "metrics_8 = calculate_metrics('Gradient Boosting Regressor',grid_search,x_train, x_test, y_train, y_test)'''"
      ],
      "metadata": {
        "id": "1Ne62O551vPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a positive business impact, I would consider the following evaluation metrics:\n",
        "\n",
        "Test_MAE: Mean Absolute Error (MAE) is the average absolute difference between the predicted and actual values. It is a good metric to evaluate how well the model is predicting the target variable. In business applications, MAE can be used to evaluate the average error of the model in predicting a certain outcome. For example, if we are predicting the number of sales, we can use MAE to evaluate how accurate the model is in predicting the number of sales.\n",
        "\n",
        "Test_RMSE: Root Mean Squared Error (RMSE) is another metric used to evaluate the performance of a regression model. It is similar to MAE but takes the square root of the average squared differences between predicted and actual values. RMSE is a good metric to evaluate how well the model is predicting the target variable with respect to the scale of the target variable.\n",
        "\n",
        "Test_R2: R-squared (R2) is a metric that measures how well the model fits the data. It is the proportion of the variance in the target variable that is explained by the model. R2 is a good metric to evaluate how well the model is capturing the variation in the target variable. In business applications, R2 can be used to evaluate how well the model is capturing the underlying relationship between variables.\n",
        "\n",
        "These metrics are useful in evaluating the performance of the model and can be used to make business decisions based on the predictions made by the model.\n"
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I would chose Ridge regression as my final prediction model.\n",
        "\n",
        "There are several advantages to choosing Ridge Regression as a final prediction model:\n",
        "\n",
        "Ridge Regression is a regularized linear regression technique that can help prevent overfitting, which is a common problem in machine learning.\n",
        "\n",
        "It can handle a large number of features, and even features that are correlated with each other.\n",
        "\n",
        "Ridge Regression can perform well in situations where there is a small amount of training data available.\n",
        "\n",
        "It has a closed-form solution, which means that it can be solved analytically and doesn't require expensive iterative optimization algorithms.\n",
        "\n",
        "Ridge Regression is also relatively simple and easy to understand, which can be useful for explaining the model to non-technical stakeholders.\n",
        "\n",
        "Based on these advantages, Ridge Regression can be a good choice for a final prediction model in many situations."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model used in this code is a Ridge regression model, which is a type of linear regression that includes a regularization term in order to prevent overfitting. The ridge object is the trained Ridge regression model.\n",
        "\n",
        "The code then uses the SHAP (SHapley Additive exPlanations) library to explain the predictions made by the model. SHAP values help to explain how the features contribute to the model's predictions on a per-instance basis. The LinearExplainer function is used to create an explainer object that can calculate the SHAP values for a given set of features. The shap_values variable then stores the SHAP values for the test data, which are calculated using the shap_values function of the explainer object.\n",
        "\n",
        "Finally, the code uses the summary_plot function from the SHAP library to plot a summary plot of the feature importance. This plot shows the magnitude and direction of each feature's effect on the model's output, and it provides an intuitive way to see which features are the most important for making predictions. The feature_names argument is used to label the features in the plot with their corresponding names."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install shap"
      ],
      "metadata": {
        "id": "6GgraAc0lJ1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shap"
      ],
      "metadata": {
        "id": "-1wj6_nflO9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a SHAP explainer object with the trained model\n",
        "explainer = shap.LinearExplainer(ridge_best, x_train)\n",
        "\n",
        "# Calculate the SHAP values for the test data\n",
        "shap_values = explainer.shap_values(x_test)\n",
        "\n",
        "# Create a summary plot to show the feature importance as a horizontal bar chart\n",
        "shap.summary_plot(shap_values, x_test, feature_names=index, plot_type='bar', color='b', sort=True)"
      ],
      "metadata": {
        "id": "3m0zi3AplUnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " customers and weekofyear were found to be the top two important features in the SHAP summary plot, it suggests that these features have the strongest impact on the target variable (sales). Specifically, an increase in the number of customers or in a specific week of the year is likely to lead to an increase in sales.\n",
        "\n",
        "This information can be used by businesses to make data-driven decisions and adjust their strategies accordingly. For example, if a business notices that sales are consistently higher during certain weeks of the year, they could adjust their marketing or promotions during those times to further increase sales. Additionally, if a business is looking to increase sales, they could focus on ways to increase the number of customers they have, such as through advertising or improving customer service."
      ],
      "metadata": {
        "id": "wbTDhkfzUhAU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#cocatinating all the model's metrices\n",
        "score_df = pd.concat([metrics_2,metrics_3,metrics_6,metrics_7]).reset_index()"
      ],
      "metadata": {
        "id": "0lD2ZblhAoBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score_df"
      ],
      "metadata": {
        "id": "VbvDBcspD_Ua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the analysis, the following conclusions can be drawn:\n",
        "\n",
        "There is a strong positive correlation between store sales and the number of customers visiting the store. This suggests that if a store can increase the number of customers, it will likely increase its sales as well.\n",
        "\n",
        "Stores with larger assortment sizes tend to have higher sales. This implies that increasing the variety of products a store offers could lead to increased sales.\n",
        "\n",
        "Promotions such as Sales and holidays tend to increase the sales of the store, as customers tend to buy more items during those days.\n",
        "\n",
        "The analysis also revealed that the day of the week has an impact on sales, with Sundays and Mondays having the lowest sales. This information can be useful in scheduling employee hours and managing inventory levels.\n",
        "\n",
        "The competition level of nearby stores affects the sales of a store. Stores with a higher number of nearby competitors tend to have lower sales. This suggests that stores should take the competitive landscape into consideration when making business decisions.\n",
        "\n",
        "In summary, the regression analysis provides insights that can help optimize store operations and boost sales. By leveraging the key factors that impact sales, store owners can make data-driven decisions to improve their business.."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}